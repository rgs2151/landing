---
layout: default
title: "Bayesian Modeling - Part 3: Mixture Models and Hidden Structure"
date: 2025-01-17
tag: thoughts
---

Now suppose our generative process has changed. Previously, we assumed it generated data from a single continuous distribution. However, now the data appears to come from multiple **subpopulations** or **components**, and our goal is to model this situation. This scenario is best described by **Mixture Models**.

You can think of it as a bag with multiple bags inside it, each containing different preferences for balls of different colors.

*[Image Caption: A large transparent bag containing three smaller bags - one with predominantly red balls, one with blue balls, and one with mixed colors, illustrating the concept of mixture components]*

## What is a Mixture Model?

A mixture model is a probabilistic model that assumes:
1. The observed data is drawn from a combination of **multiple distributions**, each representing a distinct **component** in the data.
2. Each data point is generated by:
   - First selecting a **component** (randomly, based on a probability distribution over the components),
   - Then drawing a sample from the distribution corresponding to the chosen component.

## Components of a Mixture Model

### Latent Variable (Component Selection)
- Let $ z_n $ represent the **latent variable** (hidden category) for the $ n $-th observation, indicating which component the observation belongs to.
- $ z_n \in \{1, 2, \dots, K\} $, where $ K $ is the number of components.

### Mixing Weights (Component Probabilities)
- Each component $ k $ is associated with a probability $ \pi_k $, called the **mixing weight**, representing how likely a sample belongs to this component.
- Mixing weights satisfy:
  $$
  \sum_{k=1}^K \pi_k = 1 \quad \text{and} \quad \pi_k > 0 \, \forall k.
  $$

**note**: $\forall$ means "for all" :)

### Component Distributions
- Each component $ k $ has a distribution (e.g., Gaussian, Poisson, etc.) with its own parameters $ \theta_k $. For instance, in the case of a Gaussian Mixture Model (GMM), $ \theta_k = (\mu_k, \sigma_k) $.

## The Probabilistic Structure

The generative process for each data point $ y_n $ is as follows:
1. Draw a component $ z_n $ from the categorical distribution:
   $$
   z_n \sim \text{Categorical}(\pi_1, \pi_2, \dots, \pi_K)
   $$
2. Given $ z_n = k $, draw the observation $ y_n $ from the corresponding component distribution:
   $$
   y_n \sim f(y | \theta_k)
   $$

### Joint and Marginal Distributions

1. **Joint Probability of Observing $ (z_n, y_n) $:**
   $$
   P(z_n, y_n) = P(z_n) P(y_n | z_n)
   $$

2. **Marginal Probability of $ y_n $:**
   - By marginalizing over the latent variable $ z_n $:
   $$
   P(y_n) = \sum_{k=1}^K P(z_n = k) P(y_n | z_n = k)
   $$
   - Substituting $ P(z_n = k) = \pi_k $:
   $$
   P(y_n) = \sum_{k=1}^K \pi_k f(y_n | \theta_k)
   $$
   - **This is the most common representation of mixture models!**

Intuitively, we can visualize the mixture model as follows:

*[Image Caption: A flow diagram showing the two-step process: 1) Select component with probability π, 2) Generate observation from selected component's distribution]*

## Probabilistic Graphical Model

Below is the corresponding **probabilistic graphical model (PGM)** that illustrates the dependencies in a mixture model. The **plate notation** indicates repeated variables for $ n = 1, \dots, N $ observations and $ k = 1, \dots, K $ components.

*[Image Caption: A directed graphical model showing π pointing to z_n, μ_k pointing to y_n, and z_n pointing to y_n, with appropriate plate notations for N observations and K components]*

- $ \pi $: Mixing weights (shared across all data points).
- $ z_n $: Latent variable (which component generated the $ n $-th observation).
- $ \mu_k $: Parameters of the $ k $-th component (e.g., mean in a GMM).
- $ y_n $: Observed data.

This framework captures the **hierarchical structure**:
- $ z_n $ determines which component $ y_n $ is drawn from.
- $ \pi $ controls the probabilities of the components.
- $ \mu_k $ determines the shape of each component.

## Bayesian Specification

In the Bayesian framework, we consider the following components:

1. **Priors**:
   - $ \pi \sim \text{Dirichlet}(\alpha_1, \dots, \alpha_K) $ (mixing weights),
   - $ \theta_k \sim \text{Prior for component parameters (e.g., Gaussian)} $.

2. **Likelihood**:
   - The likelihood of observing $ y_n $, given the parameters $ \pi $ and $ \theta $, is:
   $$
   P(y_n | \pi, \theta) = \sum_{k=1}^K \pi_k f(y_n | \theta_k)
   $$

3. **Posterior**:
   - Using Bayes' theorem, the posterior distribution updates our beliefs about the parameters based on the observed data:
   $$
   P(\pi, \theta | y) \propto P(y | \pi, \theta) P(\pi) P(\theta)
   $$

## Inference in Mixture Models with PyMC

Now let's simulate a mixture model and infer the parameters using MCMC. First, let's create an interactive way to generate data:

```python
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
from scipy.stats import norm
import ipywidgets as widgets
from IPython.display import display

def get_color_gradient(n, color1="red", color2="blue"):
    cmap = mcolors.LinearSegmentedColormap.from_list("gradient", [color1, color2])
    return [cmap(i / (n - 1)) for i in range(n)]

def plot_mixture(y, imeans=None, iweights=None, show_comps=False, ax=None):
    if ax is None:
        fig, ax = plt.subplots(1, 1, figsize=(8, 3))

    # Plot the mixture
    _, _, patches = ax.hist(y, bins=26, density=True, edgecolor='black', 
                           color="skyblue", alpha=0.7, label="Data")
    for i, patch in enumerate(patches):
        patch.set_facecolor(get_color_gradient(26)[i])

    if imeans is not None and iweights is not None:
        x = np.linspace(-5, 8, 1000)

        # Plot the mixture uncertainty
        for i in range(len(iweights)):
            pdf = (iweights[i,0] * norm.pdf(x, loc=imeans[i,0], scale=1) + 
                   iweights[i, 1] * norm.pdf(x, loc=imeans[i, 1], scale=1))
            ax.plot(x, pdf, color="orange", alpha=0.003)

        # Plot the mixture mean
        mixture_density = (iweights.mean(axis=0)[0] * norm.pdf(x, imeans.mean(axis=0)[0], 1) +
                          iweights.mean(axis=0)[1] * norm.pdf(x, imeans.mean(axis=0)[1], 1))
        ax.plot(x, mixture_density, "k--", label="Mixture Mean")

    # Plot the components
    if show_comps:
        for weight, mean in zip(iweights.mean(0), imeans.mean(0)):
            component_density = weight * norm.pdf(x, mean, 1)
            ax.plot(x, component_density, linestyle="--", alpha=0.9, 
                   label=f"Component: μ={mean:.2f}")
    
    ax.set_title("Mixture of Gaussians")
    ax.set_xlabel("y")
    ax.set_ylabel("Density")
    ax.set_xlim(-5, 8)
    ax.set_ylim(0, 0.5)
    ax.legend()

# Interactive widget for generating mixture data
a = widgets.FloatSlider(value=0.7, min=0, max=1)
mean_a = widgets.FloatSlider(value=0, min=-5, max=8)
mean_b = widgets.FloatSlider(value=4, min=-5, max=8)

def update_plot(weight, mean_1, mean_2):
    components = np.random.choice(2, size=2000, p=[weight, 1-weight])
    global y
    y = np.array([np.random.normal([mean_1, mean_2][k], 1) for k in components])
    plot_mixture(y=y)

w = widgets.interactive(update_plot, weight=a, mean_1=mean_a, mean_2=mean_b)
display(w)
```

Now let's fit a Bayesian mixture model to this data:

```python
import pymc as pm

# Mixture of Normal Components
with pm.Model() as model:
    # Priors
    w = pm.Dirichlet("w", a=np.array([1, 1]))  # 2 mixture weights
    mu1 = pm.Normal("mu1", 0, 1)
    mu2 = pm.Normal("mu2", 0, 1)

    components = [
        pm.Normal.dist(mu=mu1, sigma=1),
        pm.Normal.dist(mu=mu2, sigma=1),
    ]

    like = pm.Mixture("like", w=w, comp_dists=components, observed=y)

    # Sample
    trace = pm.sample(1000, chains=1)
```

Let's examine our results:

```python
import arviz as az

# Extract posteriors
imeans = np.array([trace.posterior.mu1[0].values, trace.posterior.mu2[0].values]).T
iweights = trace.posterior.w[0].values

plot_mixture(y, imeans, iweights, show_comps=False)
ax = az.plot_posterior(trace, figsize=(20, 2), round_to=2)
for a in ax.flatten(): 
    a.set_ylabel("Density")
```

## Understanding the Results

**Notes:**

1. **Mixture Fit Results**:
   - The observed histogram shows two distinct peaks, which are well-captured by the **Mixture of Gaussians**.
   - The dashed black line represents the **mixture mean**, which combines the contributions of both components.

2. **Posterior Summaries**:
   - For $ \mu_1 $ (mean of the first component), the posterior mean is approximately **0.03**, indicating the first component centers near zero.
   - For $ \mu_2 $ (mean of the second component), the posterior mean is approximately **4.1**, reflecting the second component is centered around 4.
   - The weights $ w_0 $ and $ w_1 $ (mixing proportions) show posterior means of **0.7** and **0.3**, respectively, indicating the first component contributes 70% and the second contributes 30% to the mixture.

3. **Uncertainty in Estimation**:
   - The credible intervals (94% HDI) for all parameters are narrow, showing high confidence in the estimates for means ($ \mu_1, \mu_2 $) and weights ($ w_0, w_1 $).

**Quiz**: Mixture Peaks - From the plot, why does the first peak appear taller than the second? How do the weights influence this observation?

## Key Insights About Mixture Models

1. **Hidden Structure**: Mixture models help us discover hidden subgroups in our data
2. **Automatic Clustering**: The model automatically assigns data points to components
3. **Flexible Modeling**: We can model complex, multimodal distributions
4. **Uncertainty in Assignment**: Unlike hard clustering, we get probabilities of membership

## What's Next?

In Part 4, we'll introduce **temporal dependencies** with **Markov Models**. What happens when the choice of which bag to sample from depends on which bag we used in the previous step? This leads us into the fascinating world of Hidden Markov Models!

Think of it as a sequence of bags, where the probability of choosing the next bag depends on which bag you're currently using. This temporal structure opens up entirely new modeling possibilities.

---

*Continue to [Part 4: Markov Models and Temporal Dependencies](../bayesian-tutorial-part4) to learn about modeling sequential data with hidden states.*
