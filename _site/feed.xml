<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-07-22T23:10:47-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Rudramani Singha</title><subtitle>Hey there! I&apos;m a researcher based in New York City.  I build probabilistic models and reverse engineer the brain.</subtitle><author><name>Rudramani Singha</name></author><entry><title type="html">Hypothesis Testing vs Bayesian Modeling</title><link href="http://localhost:4000/posts/hypothesis/" rel="alternate" type="text/html" title="Hypothesis Testing vs Bayesian Modeling" /><published>2025-03-12T21:27:20-04:00</published><updated>2025-03-12T21:27:20-04:00</updated><id>http://localhost:4000/posts/hypothesis</id><content type="html" xml:base="http://localhost:4000/posts/hypothesis/"><![CDATA[<p>“How does this compare to the classical statistics I learned in school?” This is a fundamental question that gets to the heart of two very different philosophies about uncertainty, evidence, and decision-making.</p>

<p>Let me tell you a story that illustrates the key differences…</p>

<h2 id="the-tale-of-two-scientists">The Tale of Two Scientists</h2>

<p>Imagine two scientists studying whether a new drug is effective. They both have the same data, but they approach the problem completely differently.</p>

<h3 id="dr-frequentists-approach">Dr. Frequentist’s Approach</h3>

<p>Dr. Frequentist sets up her analysis like a court trial:</p>

<p><strong>The Setup:</strong></p>
<ul>
  <li><strong>Null Hypothesis (H₀):</strong> “The drug has no effect” (innocent until proven guilty)</li>
  <li><strong>Alternative Hypothesis (H₁):</strong> “The drug has an effect”</li>
  <li><strong>Goal:</strong> Find evidence strong enough to reject H₀</li>
</ul>

<p>She calculates a test statistic and gets a p-value of 0.03.</p>

<p><strong>Her Conclusion:</strong> “If the drug truly had no effect, there’s only a 3% chance I’d see data this extreme or more extreme. Since 3% &lt; 5%, I reject the null hypothesis. The drug is effective.”</p>

<p><strong>What she CAN’T say:</strong> “There’s a 97% chance the drug works” (This is a common misinterpretation!)</p>

<h3 id="dr-bayesians-approach">Dr. Bayesian’s Approach</h3>

<p>Dr. Bayesian thinks differently:</p>

<p><strong>The Setup:</strong></p>
<ul>
  <li><strong>Prior:</strong> Based on previous studies, she believes there’s a 30% chance the drug works</li>
  <li><strong>Likelihood:</strong> She models how likely the observed data is under different effect sizes</li>
  <li><strong>Posterior:</strong> She updates her beliefs using Bayes’ theorem</li>
</ul>

<p><strong>Her Process:</strong>
\(P(\text{Drug Works | Data}) = \frac{P(\text{Data | Drug Works}) \times P(\text{Drug Works})}{P(\text{Data})}\)</p>

<p>After seeing the data, her posterior shows an 85% probability that the drug works.</p>

<p><strong>Her Conclusion:</strong> “Given the data and my prior knowledge, I’m now 85% confident the drug is effective. Here’s the full distribution of possible effect sizes…”</p>

<h2 id="the-fundamental-philosophical-differences">The Fundamental Philosophical Differences</h2>

<h3 id="1-what-is-probability">1. <strong>What is Probability?</strong></h3>

<p><strong>Frequentist View:</strong> Probability is about long-run frequencies. “If I repeated this experiment infinite times under the same conditions, what fraction would give this result?”</p>

<p><strong>Bayesian View:</strong> Probability is about degrees of belief or uncertainty. “Given what I know, how confident am I in this statement?”</p>

<h3 id="2-what-questions-can-we-answer">2. <strong>What Questions Can We Answer?</strong></h3>

<p><strong>Frequentist:</strong></p>
<ul>
  <li>✅ “What’s the probability of seeing this data if H₀ is true?” (p-value)</li>
  <li>❌ “What’s the probability that H₀ is true?” (Not allowed!)</li>
</ul>

<p><strong>Bayesian:</strong></p>
<ul>
  <li>✅ “What’s the probability that H₀ is true given the data?” (Posterior probability)</li>
  <li>✅ “What are all the possible parameter values and how likely are they?”</li>
</ul>

<h3 id="3-how-do-we-handle-prior-knowledge">3. <strong>How Do We Handle Prior Knowledge?</strong></h3>

<p><strong>Frequentist:</strong> Prior knowledge is largely ignored in the formal analysis. Each study stands alone.</p>

<p><strong>Bayesian:</strong> Prior knowledge is explicitly incorporated and updated with new evidence.</p>

<h2 id="a-concrete-example-the-coin-flipping-dilemma">A Concrete Example: The Coin Flipping Dilemma</h2>

<p>Let’s say you flip a coin 10 times and get 8 heads. Is this a fair coin?</p>

<h3 id="frequentist-analysis">Frequentist Analysis</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Observed: 8 heads out of 10 flips
# H₀: p = 0.5 (fair coin)
# H₁: p ≠ 0.5 (unfair coin)
</span>
<span class="n">observed_heads</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">n_flips</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">null_p</span> <span class="o">=</span> <span class="mf">0.5</span>

<span class="c1"># Two-tailed binomial test
</span><span class="n">p_value</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="p">.</span><span class="n">binom</span><span class="p">.</span><span class="nf">cdf</span><span class="p">(</span><span class="n">observed_heads</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_flips</span><span class="p">,</span> <span class="n">null_p</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">P-value: </span><span class="si">{</span><span class="n">p_value</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Result: p-value = 0.1094
# Conclusion: Fail to reject H₀ (not enough evidence to say it's unfair)
</span></code></pre></div></div>

<p><strong>Frequentist says:</strong> “We can’t conclude the coin is unfair (p = 0.11 &gt; 0.05).”</p>

<h3 id="bayesian-analysis">Bayesian Analysis</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">scipy</span> <span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Prior: Uniform distribution (all probabilities equally likely)
# This is equivalent to Beta(1, 1)
</span><span class="n">alpha_prior</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">beta_prior</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># Data: 8 heads, 2 tails
</span><span class="n">heads</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">tails</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># Posterior: Beta distribution (conjugate prior magic!)
</span><span class="n">alpha_posterior</span> <span class="o">=</span> <span class="n">alpha_prior</span> <span class="o">+</span> <span class="n">heads</span>
<span class="n">beta_posterior</span> <span class="o">=</span> <span class="n">beta_prior</span> <span class="o">+</span> <span class="n">tails</span>

<span class="c1"># The posterior is Beta(9, 3)
</span><span class="n">posterior</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="nf">beta</span><span class="p">(</span><span class="n">alpha_posterior</span><span class="p">,</span> <span class="n">beta_posterior</span><span class="p">)</span>

<span class="c1"># Plot the results
</span><span class="n">p_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">prior_density</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="nf">beta</span><span class="p">(</span><span class="n">alpha_prior</span><span class="p">,</span> <span class="n">beta_prior</span><span class="p">).</span><span class="nf">pdf</span><span class="p">(</span><span class="n">p_values</span><span class="p">)</span>
<span class="n">posterior_density</span> <span class="o">=</span> <span class="n">posterior</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">p_values</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">p_values</span><span class="p">,</span> <span class="n">prior_density</span><span class="p">,</span> <span class="sh">'</span><span class="s">b--</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Prior belief</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">p_values</span><span class="p">,</span> <span class="n">posterior_density</span><span class="p">,</span> <span class="sh">'</span><span class="s">r-</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Posterior belief</span><span class="sh">'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">black</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">:</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Fair coin (p=0.5)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Probability of Heads</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Density</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Bayesian Coin Analysis: Before and After Data</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="c1"># Calculate probabilities
</span><span class="n">prob_unfair</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">posterior</span><span class="p">.</span><span class="nf">cdf</span><span class="p">(</span><span class="mf">0.6</span><span class="p">)</span> <span class="o">+</span> <span class="n">posterior</span><span class="p">.</span><span class="nf">cdf</span><span class="p">(</span><span class="mf">0.4</span><span class="p">)</span>
<span class="n">prob_very_unfair</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">posterior</span><span class="p">.</span><span class="nf">cdf</span><span class="p">(</span><span class="mf">0.7</span><span class="p">)</span> <span class="o">+</span> <span class="n">posterior</span><span class="p">.</span><span class="nf">cdf</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Probability coin is unfair (p &lt; 0.4 or p &gt; 0.6): </span><span class="si">{</span><span class="n">prob_unfair</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Probability coin is very unfair (p &lt; 0.3 or p &gt; 0.7): </span><span class="si">{</span><span class="n">prob_very_unfair</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Most likely value of p: </span><span class="si">{</span><span class="n">posterior</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">95% credible interval: [</span><span class="si">{</span><span class="n">posterior</span><span class="p">.</span><span class="nf">ppf</span><span class="p">(</span><span class="mf">0.025</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">posterior</span><span class="p">.</span><span class="nf">ppf</span><span class="p">(</span><span class="mf">0.975</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">]</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Bayesian says:</strong> “There’s a 67% chance the coin is significantly unfair, and the most likely probability of heads is 0.75. Here’s my full uncertainty about all possible values…”</p>

<h2 id="when-to-use-which-approach">When to Use Which Approach?</h2>

<h3 id="use-frequentist-methods-when">Use <strong>Frequentist Methods</strong> When:</h3>
<ul>
  <li>You need regulatory approval (FDA, etc.) - they often require frequentist approaches</li>
  <li>You want to control false positive rates in multiple testing scenarios</li>
  <li>You have no meaningful prior information</li>
  <li>You need simple, standardized procedures that others can easily replicate</li>
  <li>You’re doing exploratory data analysis where you want to “let the data speak”</li>
</ul>

<h3 id="use-bayesian-methods-when">Use <strong>Bayesian Methods</strong> When:</h3>
<ul>
  <li>You have genuine prior information that should influence the analysis</li>
  <li>You want to quantify uncertainty about parameters (not just reject/accept hypotheses)</li>
  <li>You need to make optimal decisions under uncertainty</li>
  <li>You want interpretable probability statements about your hypotheses</li>
  <li>You’re building predictive models</li>
  <li>You have complex, hierarchical data structures</li>
  <li>Sample sizes are small and every bit of information matters</li>
</ul>

<h2 id="the-integration-perspective">The Integration Perspective</h2>

<p>Here’s the thing: these approaches aren’t always mutually exclusive. Modern statistics increasingly recognizes that:</p>

<ol>
  <li><strong>Both have their place</strong>: Different questions call for different tools</li>
  <li><strong>Bayesian methods can be more intuitive</strong>: They answer the questions we actually want to ask</li>
  <li><strong>Frequentist methods provide important guarantees</strong>: They control error rates in repeated sampling</li>
  <li><strong>The gap is narrowing</strong>: Modern computational methods make Bayesian analysis more accessible</li>
</ol>

<h2 id="a-personal-take">A Personal Take</h2>

<p>After working extensively with both approaches, I find myself gravitating toward Bayesian methods for most real-world problems. Here’s why:</p>

<p><strong>Bayesian modeling feels more honest about uncertainty.</strong> Instead of binary reject/don’t-reject decisions, you get nuanced probability distributions that capture what you actually know and don’t know.</p>

<p><strong>It’s more flexible for complex problems.</strong> Once you understand the Bayesian framework, you can tackle mixture models, hierarchical structures, and missing data in ways that frequentist methods struggle with.</p>

<p><strong>It answers the questions we actually care about.</strong> When someone asks “What’s the probability this treatment works?”, Bayesian analysis can give a direct answer.</p>

<p>But here’s the key insight: <strong>The best approach depends on your specific problem, your audience, and your goals.</strong></p>

<h2 id="moving-forward">Moving Forward</h2>

<p>If you’re coming from a traditional statistics background, I encourage you to:</p>

<ol>
  <li><strong>Start with simple Bayesian analyses</strong> on problems you understand well</li>
  <li><strong>Compare results</strong> between frequentist and Bayesian approaches</li>
  <li><strong>Focus on interpretation</strong> - what do the results actually mean?</li>
  <li><strong>Think about your priors</strong> - what do you actually believe before seeing the data?</li>
</ol>

<p>The future of statistics isn’t about choosing sides in some philosophical war. It’s about understanding both approaches deeply enough to choose the right tool for each specific problem.</p>

<p>And honestly? Once you start thinking like a Bayesian, it’s hard to go back. The world just makes more sense when you can explicitly model your uncertainty and update your beliefs as evidence accumulates.</p>

<hr />

<p><em>This post builds on concepts from our <a href="../bayesian-tutorial">Bayesian Modeling series</a>. If you haven’t checked it out yet, it provides a hands-on introduction to Bayesian thinking with practical examples.</em></p>]]></content><author><name>Rudramani Singha</name></author><category term="thoughts" /><summary type="html"><![CDATA[“How does this compare to the classical statistics I learned in school?” This is a fundamental question that gets to the heart of two very different philosophies about uncertainty, evidence, and decision-making.]]></summary></entry><entry><title type="html">Bayesian Modeling Tutorial</title><link href="http://localhost:4000/posts/bayesian-tutorial/" rel="alternate" type="text/html" title="Bayesian Modeling Tutorial" /><published>2025-01-15T00:00:00-05:00</published><updated>2025-01-15T00:00:00-05:00</updated><id>http://localhost:4000/posts/bayesian-tutorial</id><content type="html" xml:base="http://localhost:4000/posts/bayesian-tutorial/"><![CDATA[<p>Welcome to our comprehensive Bayesian modeling tutorial! We’ll start with the fundamentals and work our way up to complex models. Think of this as your journey from understanding basic probability to building sophisticated inference systems.</p>

<h2 id="part-1-from-balls-in-bags-to-probability">Part 1: From Balls in Bags to Probability</h2>

<h3 id="11-probability-and-quantifying-variability">1.1 Probability and Quantifying Variability</h3>

<p>Probability is defined as a number between 0 and 1, which describes the likelihood of the occurrence of some particular event in some range of events. 0 means an infinitely unlikely event, and 1 means the certain event. The term ‘event’ is very general, but for us can usually be thought of as one sample data point in an experiment, and the ‘range of events’ would be all the data we would get if we sampled virtually an infinite dataset on the experiment.</p>

<p>Imagine a bag with red and blue balls:</p>

<p><em>[Image Caption: A transparent bag containing 6 red balls and 6 blue balls, showing equal distribution]</em></p>

<p>6 red balls and 6 blue balls.</p>

<p>If you randomly “sample” one ball (a.k.a. event), the probability of getting a red ball is the fraction of red balls to the total.</p>

\[P(Red)= \frac {\text{Number of Red Balls}​} {\text{Total Number of Balls}}\]

<p>Similarly, the probability of getting a blue ball is the fraction of blue balls to the total.</p>

\[P(Blue)= \frac {\text{Number of Blue Balls}​} {\text{Total Number of Balls}}\]

<p>And the sum of the probabilities of all possible outcomes would be 1.</p>

\[P(Red) + P(Blue) = 1\]

<h3 id="the-generative-process">The Generative Process</h3>

<p>If you sampled a ball and put it back in the bag, the probability of getting a red ball would be the same on the next draw. However, if you sampled a ball and didn’t put it back in the bag, the probability of getting a red ball would change for the next draw.</p>

<p><strong>For the rest of this tutorial, we will assume that the generative process that makes these balls can infinitely keep making balls and we can draw as much as we want to discover the true distribution.</strong></p>

<p>With sufficient draws you can be more and more confident about the variability in the probability and the true distribution in the generative process. With a large dataset, the sample probability will converge to the true probability which should be .5/.5 in this case.</p>

<p>Here’s a simple demonstration of this convergence:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Show the probability converging to .5 .5 with increasing number of samples
</span><span class="n">n_draws</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">]</span>
<span class="n">color</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">#ff4c4c</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">#4c4cff</span><span class="sh">"</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">n_draws</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">([</span><span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">],</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">data</span> <span class="o">==</span> <span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">var</span> <span class="o">=</span> <span class="n">p</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">bar</span><span class="p">([</span><span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">],</span> <span class="p">[</span><span class="n">p</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">errorbar</span><span class="p">([</span><span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">],</span> <span class="p">[</span><span class="n">p</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">],</span> <span class="n">yerr</span><span class="o">=</span><span class="n">var</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="sh">"</span><span class="s">none</span><span class="sh">"</span><span class="p">,</span> <span class="n">ecolor</span><span class="o">=</span><span class="sh">"</span><span class="s">black</span><span class="sh">"</span><span class="p">,</span> <span class="n">capsize</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p><strong><em>Run this code multiple times and see which graph changes most dramatically between runs!</em></strong></p>

<h3 id="12-updating-beliefs-with-new-data">1.2 Updating Beliefs with New Data</h3>

<p>As you intuitively understood from the above experiment, as you draw more and more samples, you can be more and more confident about the true distribution of the balls in the bag. This is the <strong>basis of Bayesian statistics</strong>.</p>

<p>We can formalize this process with Bayes’ theorem:</p>

\[P(\text{Red | Data}) = \frac{P(\text{Data | Red}) P(\text{Red})}{P(\text{Data})}\]

<p>Where:</p>
<ul>
  <li>$ P(\text{Red}) $: Prior belief about the proportion of red balls.</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$ P(\text{Data</td>
          <td>Red}) $: Likelihood of observing the data given the proportion of red balls.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>$ P(\text{Data}) $: Total probability of observing the data.</li>
</ul>

<p>So, basically, as you acquired more data, you updated your belief about the proportion of red balls in the bag (which in this case is the same as the probability).</p>

<h3 id="13-from-discrete-to-continuous-space">1.3 From Discrete to Continuous Space</h3>

<p><strong>Notice</strong> how you could either get a red ball or a blue ball. This means your <strong>choices are discrete.</strong></p>

<p>Sometimes, it is possible that the generative process is <strong>continuous</strong>. Such as, what if the bag had balls in a <strong>spectrum of colors</strong> between red and blue and till now you were only sampling from a specific subset of the dataset. And now for some unknown reason, the generative process has changed and you are now sampling from the whole dataset.</p>

<p><em>[Image Caption: A bag showing balls in a continuous spectrum from deep blue through purple to deep red, representing continuous color variation]</em></p>

<p>To discover this new true distribution of the generative process, we make histograms of the samples we draw. By making the buckets in the histograms thinner and thinner (to infinity), we can get a better idea of the <strong><em>true continuous distribution</em></strong>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="kn">import</span> <span class="n">matplotlib.colors</span> <span class="k">as</span> <span class="n">mcolors</span>

<span class="k">def</span> <span class="nf">get_color_gradient</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">color1</span><span class="o">=</span><span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">,</span> <span class="n">color2</span><span class="o">=</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">cmap</span> <span class="o">=</span> <span class="n">mcolors</span><span class="p">.</span><span class="n">LinearSegmentedColormap</span><span class="p">.</span><span class="nf">from_list</span><span class="p">(</span><span class="sh">"</span><span class="s">gradient</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="n">color1</span><span class="p">,</span> <span class="n">color2</span><span class="p">])</span>
    <span class="k">return</span> <span class="p">[</span><span class="nf">cmap</span><span class="p">(</span><span class="n">i</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>

<span class="c1"># Normal distribution parameters
</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span>
<span class="n">num_bins_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">26</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">create_pmf</span><span class="p">(</span><span class="n">num_bins</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">num_bins</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">bin_centers</span> <span class="o">=</span> <span class="p">(</span><span class="n">bins</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">bins</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">pmf_values</span> <span class="o">=</span> <span class="n">norm</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">bin_centers</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">bins</span><span class="p">,</span> <span class="n">bin_centers</span><span class="p">,</span> <span class="n">pmf_values</span>

<span class="k">def</span> <span class="nf">plot_approximation</span><span class="p">(</span><span class="n">num_bins_list</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">num_bins_list</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">num_bins</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">num_bins_list</span><span class="p">):</span>
        <span class="n">bins</span><span class="p">,</span> <span class="n">bin_centers</span><span class="p">,</span> <span class="n">pmf_values</span> <span class="o">=</span> <span class="nf">create_pmf</span><span class="p">(</span><span class="n">num_bins</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
        <span class="n">gradient_colors</span> <span class="o">=</span> <span class="nf">get_color_gradient</span><span class="p">(</span><span class="n">num_bins</span><span class="p">,</span> <span class="n">color1</span><span class="o">=</span><span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">,</span> <span class="n">color2</span><span class="o">=</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">)</span>

        <span class="n">ax</span><span class="p">.</span><span class="nf">bar</span><span class="p">(</span><span class="n">bin_centers</span><span class="p">,</span> <span class="n">pmf_values</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="p">(</span><span class="n">bins</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">bins</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> 
               <span class="n">color</span><span class="o">=</span><span class="n">gradient_colors</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="sh">'</span><span class="s">black</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">center</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">bin_centers</span><span class="p">,</span> <span class="n">pmf_values</span><span class="p">,</span> <span class="n">gradient_colors</span><span class="p">):</span>
            <span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">center</span><span class="p">,</span> <span class="n">value</span> <span class="o">+</span> <span class="mf">0.05</span><span class="p">,</span> <span class="sh">'</span><span class="s">o</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

        <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">PMF Approximation with </span><span class="si">{</span><span class="n">num_bins</span><span class="si">}</span><span class="s"> Balls</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Probability (Mass Density)</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="nf">set_xticks</span><span class="p">(</span><span class="n">bin_centers</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="nf">set_xticklabels</span><span class="p">([</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="nf">chr</span><span class="p">(</span><span class="mi">97</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_bins</span><span class="p">)])</span>
        <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Smaller bin sizes for better Approximations</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">plot_approximation</span><span class="p">(</span><span class="n">num_bins_list</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
</code></pre></div></div>

<p>Transitioning from a discrete probability to a continuous probability involves shifting from assigning probabilities to distinct, separate outcomes to describing probabilities across a continuum of possibilities.</p>

<p><strong>We need a way to model this continuous distribution!</strong></p>

<p>To do this, we assume that the histogram is an approximation of a continuous function which describes a probability distribution. And we map our histogram to the “best suited” continuous function hoping that this is the true distribution of the generative process.</p>

<p>Below are some examples of different continuous functions that can be used to approximate the histogram:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span><span class="p">,</span> <span class="n">poisson</span><span class="p">,</span> <span class="n">expon</span><span class="p">,</span> <span class="n">uniform</span><span class="p">,</span> <span class="n">halfnorm</span><span class="p">,</span> <span class="n">beta</span>

<span class="n">x_values</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">Normal</span><span class="sh">"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1000</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">Poisson</span><span class="sh">"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">Exponential</span><span class="sh">"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1000</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">Uniform</span><span class="sh">"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">Half-Normal</span><span class="sh">"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1000</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">Beta</span><span class="sh">"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">),</span>
<span class="p">}</span>

<span class="n">pdfs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">Normal</span><span class="sh">"</span><span class="p">:</span> <span class="n">norm</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x_values</span><span class="p">[</span><span class="sh">"</span><span class="s">Normal</span><span class="sh">"</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">Poisson</span><span class="sh">"</span><span class="p">:</span> <span class="n">poisson</span><span class="p">.</span><span class="nf">pmf</span><span class="p">(</span><span class="n">x_values</span><span class="p">[</span><span class="sh">"</span><span class="s">Poisson</span><span class="sh">"</span><span class="p">].</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span> <span class="n">mu</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">Exponential</span><span class="sh">"</span><span class="p">:</span> <span class="n">expon</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x_values</span><span class="p">[</span><span class="sh">"</span><span class="s">Exponential</span><span class="sh">"</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">Uniform</span><span class="sh">"</span><span class="p">:</span> <span class="n">uniform</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x_values</span><span class="p">[</span><span class="sh">"</span><span class="s">Uniform</span><span class="sh">"</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">Half-Normal</span><span class="sh">"</span><span class="p">:</span> <span class="n">halfnorm</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x_values</span><span class="p">[</span><span class="sh">"</span><span class="s">Half-Normal</span><span class="sh">"</span><span class="p">]),</span>
    <span class="sh">"</span><span class="s">Beta</span><span class="sh">"</span><span class="p">:</span> <span class="n">beta</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x_values</span><span class="p">[</span><span class="sh">"</span><span class="s">Beta</span><span class="sh">"</span><span class="p">],</span> <span class="n">a</span><span class="o">=</span><span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="p">.</span><span class="mi">5</span><span class="p">),</span>
<span class="p">}</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">colors</span> <span class="o">=</span> <span class="nf">get_color_gradient</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">pdfs</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">dist_name</span><span class="p">,</span> <span class="n">x_vals</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">x_values</span><span class="p">.</span><span class="nf">items</span><span class="p">()):</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">pdfs</span><span class="p">[</span><span class="n">dist_name</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">dist_name</span><span class="si">}</span><span class="s"> PDF/PMF</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">dist_name</span><span class="si">}</span><span class="s"> Distribution</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Value</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Density / Probability</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p><strong><em>So which function should we choose?</em></strong></p>

<h1 id="part-2-bayesian-thinking-and-model-specification">Part 2: Bayesian Thinking and Model Specification</h1>

<p>Now that we understand basic probability and continuous distributions, let’s dive into the heart of Bayesian modeling. We’ll learn how to think like a Bayesian and build our first complete model.</p>

<h2 id="probability-interpretation-two-schools-of-thought">Probability Interpretation: Two Schools of Thought</h2>

<p>Choosing the right distribution is a question of what fits the data best. <strong>When we are approximating the data, we are fitting a model!</strong></p>

<p>There are 2 schools of thought on this:</p>

<ol>
  <li>
    <p><strong>Frequentist</strong>: They consider the data as fixed (given the experiment) and view the model parameters as unknown but fixed quantities. They use methods like Maximum Likelihood Estimation (MLE) to estimate the parameters that maximize the likelihood of observing the data.</p>
  </li>
  <li>
    <p><strong>Bayesian</strong>: They consider the data as observed, and the <strong>model parameters as random variables with prior beliefs</strong>. They use <strong>Bayes’ theorem to update their prior beliefs</strong> about the parameters into posterior distributions based on the observed data.</p>
  </li>
</ol>

<p><strong>For the purpose of this tutorial, we will focus specifically on the Bayesian approach.</strong></p>

<h2 id="bayesian-model-specification">Bayesian Model Specification</h2>

<p><strong>Bayesian model specification involves 3 components:</strong></p>

<ol>
  <li><strong>Prior</strong>: The prior distribution describes the beliefs about the model parameters before observing the data. It is often assumed to be a normal distribution.</li>
  <li><strong>Likelihood</strong>: The likelihood function describes the probability of observing the data given the model parameters. It is the foundation of the model.</li>
  <li><strong>Posterior</strong>: The posterior distribution describes the updated beliefs about the model parameters after observing the data. It is calculated using Bayes’ theorem.</li>
</ol>

<p>We can update our beliefs about the generative process by using the posterior distribution:</p>

\[Posterior = \frac{Likelihood \times Prior}{Evidence}\]

<h3 id="our-first-bayesian-model">Our First Bayesian Model</h3>

<p><strong>For our example of the balls in the bag, we can specify the model as follows:</strong></p>

<p><strong>Prior</strong>:</p>

<p>\(\mu \sim \mathcal{N}(0, 1)\) 
\(\sigma \sim |\mathcal{N}(0, 1)|\)</p>

<p><strong>Likelihood</strong>:</p>

\[y \sim \mathcal{N}( \mu, \sigma^2)\]

<p><strong>Posterior</strong>:</p>

\[P(\mu, \sigma | y) = \frac{P(y | \mu, \sigma) P(\mu) P(\sigma)}{P(y)}\]

<p><strong>Where</strong>:</p>
<ul>
  <li>$y$ is the data (balls drawn from the bag).</li>
  <li>$\mu$ is the mean of the distribution.</li>
  <li>$\sigma$ is the standard deviation of the distribution.</li>
  <li>$\mathcal{N}$ is the normal distribution.</li>
</ul>

<p>A nice way to visualize this is through a simple graphical model:</p>

<p><em>[Image Caption: A directed graphical model showing μ and σ as parent nodes pointing to y, with a plate notation indicating N observations y₁, y₂, …, yₙ]</em></p>

<p>This indicates that there are $ N $ independent and identically distributed (i.i.d.) observations $ y_1, y_2, \dots, y_N $.</p>

<h2 id="inference-solving-the-bayesian-model">Inference: Solving the Bayesian Model</h2>

<p>Yayy, now that we have everything, <em>LETS GO AND SOLVE IT!!!!</em></p>

<p>But wait, how do we solve it?</p>

<table>
  <tbody>
    <tr>
      <td>Solving means calculating the posterior distribution $P(\mu, \sigma</td>
      <td>y)$.</td>
    </tr>
  </tbody>
</table>

<ul>
  <li><strong>Analytically</strong>: We can solve the posterior distribution analytically for simple models such as this.</li>
  <li><strong>Numerically</strong>: For complex models, it is often <strong>intractable to solve</strong> the posterior distribution analytically. In such cases, we can use numerical methods like <strong>Markov Chain Monte Carlo (MCMC)</strong> or <strong>Variational Inference (VI)</strong> to approximate the posterior distribution.</li>
</ul>

<p><strong>For the purpose of this tutorial, we will use MCMC to approximate the posterior distribution.</strong></p>

<h3 id="implementing-our-first-model-with-stan">Implementing Our First Model with Stan</h3>

<p>Let’s implement and solve our first Bayesian model using Stan:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">tempfile</span>
<span class="kn">from</span> <span class="n">cmdstanpy</span> <span class="kn">import</span> <span class="n">CmdStanModel</span>
<span class="kn">import</span> <span class="n">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Define the Stan model
</span><span class="n">model_specification</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">
data {
    int&lt;lower=0&gt; N; // Number of data points
    array[N] real y; // Data
}
parameters {
    real mu; // Mean
    real&lt;lower=0&gt; sigma; // Standard deviation
}
model {
    // Priors
    mu ~ normal(0, 1);
    sigma ~ normal(0, 1);

    // Likelihood
    y ~ normal(mu, sigma);
}
</span><span class="sh">"""</span>

<span class="c1"># Write the model to a temporary file
</span><span class="k">with</span> <span class="n">tempfile</span><span class="p">.</span><span class="nc">NamedTemporaryFile</span><span class="p">(</span><span class="n">suffix</span><span class="o">=</span><span class="sh">"</span><span class="s">.stan</span><span class="sh">"</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">,</span> <span class="n">delete</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="k">as</span> <span class="n">tmp_file</span><span class="p">:</span>
    <span class="n">tmp_file</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">model_specification</span><span class="p">)</span>
    <span class="n">tmp_stan_path</span> <span class="o">=</span> <span class="n">tmp_file</span><span class="p">.</span><span class="n">name</span>

<span class="c1"># Prepare the unknown generator data
</span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">N</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">y</span><span class="sh">"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">),</span>
<span class="p">}</span>

<span class="c1"># Compile and fit the model
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">CmdStanModel</span><span class="p">(</span><span class="n">stan_file</span><span class="o">=</span><span class="n">tmp_stan_path</span><span class="p">)</span>
<span class="n">fit</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">iter_sampling</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</code></pre></div></div>

<p>Now let’s examine our results:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">idata</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="nf">from_cmdstanpy</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span>

<span class="c1"># Plot the posterior distribution
</span><span class="n">ax</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="nf">plot_posterior</span><span class="p">(</span><span class="n">idata</span><span class="p">,</span> <span class="n">round_to</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">textsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span><span class="sh">"</span><span class="s">Posterior Distribution of Parameters</span><span class="sh">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">ax</span><span class="p">:</span> 
    <span class="n">a</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Density</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Notes</strong>:</p>
<ul>
  <li>It seems like the model has updated its prior beliefs and landed on ~1.1 for $\mu$ and ~0.9 for $\sigma$ which is very close to the true distribution of the generative process (1 $\mu$ and 1 $\sigma$).</li>
  <li>The model is uncertain about its estimation only to a very small degree. The range for $\mu$ is 1 to 1.1 and for $\sigma$ is 0.95 to 1. It is essentially pretty certain about these parameters.</li>
</ul>

<p>Let’s visualize how well our model fits the data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_color_gradient</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">color1</span><span class="o">=</span><span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">,</span> <span class="n">color2</span><span class="o">=</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">cmap</span> <span class="o">=</span> <span class="n">mcolors</span><span class="p">.</span><span class="n">LinearSegmentedColormap</span><span class="p">.</span><span class="nf">from_list</span><span class="p">(</span><span class="sh">"</span><span class="s">gradient</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="n">color1</span><span class="p">,</span> <span class="n">color2</span><span class="p">])</span>
    <span class="k">return</span> <span class="p">[</span><span class="nf">cmap</span><span class="p">(</span><span class="n">i</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>

<span class="n">num_bins</span> <span class="o">=</span> <span class="mi">26</span>
<span class="n">all_mu</span> <span class="o">=</span> <span class="n">idata</span><span class="p">.</span><span class="n">posterior</span><span class="p">.</span><span class="n">mu</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()</span>
<span class="n">all_sigma</span> <span class="o">=</span> <span class="n">idata</span><span class="p">.</span><span class="n">posterior</span><span class="p">.</span><span class="n">sigma</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Plot the data histogram
</span><span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">patches</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">y</span><span class="sh">"</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="n">num_bins</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
                        <span class="n">edgecolor</span><span class="o">=</span><span class="sh">'</span><span class="s">black</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">skyblue</span><span class="sh">"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Data</span><span class="sh">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">patches</span><span class="p">)):</span> 
    <span class="n">patches</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">set_facecolor</span><span class="p">(</span><span class="nf">get_color_gradient</span><span class="p">(</span><span class="n">num_bins</span><span class="p">)[</span><span class="n">i</span><span class="p">])</span>

<span class="c1"># Plot the uncertainty in the model
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">all_mu</span><span class="p">)):</span>
    <span class="n">pdf</span> <span class="o">=</span> <span class="n">norm</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">all_mu</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="n">all_sigma</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pdf</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">orange</span><span class="sh">"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.003</span><span class="p">)</span>

<span class="c1"># Plot the mean of the posterior predictive distribution
</span><span class="n">mean_pdf</span> <span class="o">=</span> <span class="n">norm</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">all_mu</span><span class="p">.</span><span class="nf">mean</span><span class="p">(),</span> <span class="n">scale</span><span class="o">=</span><span class="n">all_sigma</span><span class="p">.</span><span class="nf">mean</span><span class="p">())</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mean_pdf</span><span class="p">,</span> <span class="sh">"</span><span class="s">k--</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Mean Post. Predictive</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="sh">'</span><span class="s">upper left</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Posterior Predictive Distribution and Uncertainty</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Density</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">y</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="understanding-the-results">Understanding the Results</h2>

<p>The orange cloud shows the uncertainty in our model predictions - each thin orange line represents one possible parameter combination from our posterior samples. The thick dashed black line shows the average prediction.</p>

<p>This uncertainty quantification is one of the key advantages of Bayesian modeling. We don’t just get point estimates; we get full distributions that tell us how confident we should be in our predictions.</p>

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li><strong>Bayesian models have three components</strong>: Prior, Likelihood, and Posterior</li>
  <li><strong>Priors encode our beliefs</strong> before seeing data</li>
  <li><strong>Likelihoods describe</strong> how our data depends on parameters</li>
  <li><strong>Posteriors combine</strong> priors and likelihoods using Bayes’ theorem</li>
  <li><strong>MCMC helps us solve</strong> complex models numerically</li>
  <li><strong>Uncertainty quantification</strong> comes naturally in Bayesian analysis</li>
</ol>

<h1 id="part-3-mixture-models-and-hidden-structure">Part 3: Mixture Models and Hidden Structure</h1>

<p>Now suppose our generative process has changed. Previously, we assumed it generated data from a single continuous distribution. However, now the data appears to come from multiple <strong>subpopulations</strong> or <strong>components</strong>, and our goal is to model this situation. This scenario is best described by <strong>Mixture Models</strong>.</p>

<p>You can think of it as a bag with multiple bags inside it, each containing different preferences for balls of different colors.</p>

<p><em>[Image Caption: A large transparent bag containing three smaller bags - one with predominantly red balls, one with blue balls, and one with mixed colors, illustrating the concept of mixture components]</em></p>

<h2 id="what-is-a-mixture-model">What is a Mixture Model?</h2>

<p>A mixture model is a probabilistic model that assumes:</p>
<ol>
  <li>The observed data is drawn from a combination of <strong>multiple distributions</strong>, each representing a distinct <strong>component</strong> in the data.</li>
  <li>Each data point is generated by:
    <ul>
      <li>First selecting a <strong>component</strong> (randomly, based on a probability distribution over the components),</li>
      <li>Then drawing a sample from the distribution corresponding to the chosen component.</li>
    </ul>
  </li>
</ol>

<h2 id="components-of-a-mixture-model">Components of a Mixture Model</h2>

<h3 id="latent-variable-component-selection">Latent Variable (Component Selection)</h3>
<ul>
  <li>Let $ z_n $ represent the <strong>latent variable</strong> (hidden category) for the $ n $-th observation, indicating which component the observation belongs to.</li>
  <li>$ z_n \in {1, 2, \dots, K} $, where $ K $ is the number of components.</li>
</ul>

<h3 id="mixing-weights-component-probabilities">Mixing Weights (Component Probabilities)</h3>
<ul>
  <li>Each component $ k $ is associated with a probability $ \pi_k $, called the <strong>mixing weight</strong>, representing how likely a sample belongs to this component.</li>
  <li>Mixing weights satisfy:
\(\sum_{k=1}^K \pi_k = 1 \quad \text{and} \quad \pi_k &gt; 0 \, \forall k.\)</li>
</ul>

<p><strong>note</strong>: $\forall$ means “for all” :)</p>

<h3 id="component-distributions">Component Distributions</h3>
<ul>
  <li>Each component $ k $ has a distribution (e.g., Gaussian, Poisson, etc.) with its own parameters $ \theta_k $. For instance, in the case of a Gaussian Mixture Model (GMM), $ \theta_k = (\mu_k, \sigma_k) $.</li>
</ul>

<h2 id="the-probabilistic-structure">The Probabilistic Structure</h2>

<p>The generative process for each data point $ y_n $ is as follows:</p>
<ol>
  <li>Draw a component $ z_n $ from the categorical distribution:
\(z_n \sim \text{Categorical}(\pi_1, \pi_2, \dots, \pi_K)\)</li>
  <li>Given $ z_n = k $, draw the observation $ y_n $ from the corresponding component distribution:
\(y_n \sim f(y | \theta_k)\)</li>
</ol>

<h3 id="joint-and-marginal-distributions">Joint and Marginal Distributions</h3>

<ol>
  <li>
    <p><strong>Joint Probability of Observing $ (z_n, y_n) $:</strong>
\(P(z_n, y_n) = P(z_n) P(y_n | z_n)\)</p>
  </li>
  <li>
    <p><strong>Marginal Probability of $ y_n $:</strong></p>
    <ul>
      <li>By marginalizing over the latent variable $ z_n $:
\(P(y_n) = \sum_{k=1}^K P(z_n = k) P(y_n | z_n = k)\)</li>
      <li>Substituting $ P(z_n = k) = \pi_k $:
\(P(y_n) = \sum_{k=1}^K \pi_k f(y_n | \theta_k)\)</li>
      <li><strong>This is the most common representation of mixture models!</strong></li>
    </ul>
  </li>
</ol>

<p>Intuitively, we can visualize the mixture model as follows:</p>

<p><em>[Image Caption: A flow diagram showing the two-step process: 1) Select component with probability π, 2) Generate observation from selected component’s distribution]</em></p>

<h2 id="probabilistic-graphical-model">Probabilistic Graphical Model</h2>

<p>Below is the corresponding <strong>probabilistic graphical model (PGM)</strong> that illustrates the dependencies in a mixture model. The <strong>plate notation</strong> indicates repeated variables for $ n = 1, \dots, N $ observations and $ k = 1, \dots, K $ components.</p>

<p><em>[Image Caption: A directed graphical model showing π pointing to z_n, μ_k pointing to y_n, and z_n pointing to y_n, with appropriate plate notations for N observations and K components]</em></p>

<ul>
  <li>$ \pi $: Mixing weights (shared across all data points).</li>
  <li>$ z_n $: Latent variable (which component generated the $ n $-th observation).</li>
  <li>$ \mu_k $: Parameters of the $ k $-th component (e.g., mean in a GMM).</li>
  <li>$ y_n $: Observed data.</li>
</ul>

<p>This framework captures the <strong>hierarchical structure</strong>:</p>
<ul>
  <li>$ z_n $ determines which component $ y_n $ is drawn from.</li>
  <li>$ \pi $ controls the probabilities of the components.</li>
  <li>$ \mu_k $ determines the shape of each component.</li>
</ul>

<h2 id="bayesian-specification">Bayesian Specification</h2>

<p>In the Bayesian framework, we consider the following components:</p>

<ol>
  <li><strong>Priors</strong>:
    <ul>
      <li>$ \pi \sim \text{Dirichlet}(\alpha_1, \dots, \alpha_K) $ (mixing weights),</li>
      <li>$ \theta_k \sim \text{Prior for component parameters (e.g., Gaussian)} $.</li>
    </ul>
  </li>
  <li><strong>Likelihood</strong>:
    <ul>
      <li>The likelihood of observing $ y_n $, given the parameters $ \pi $ and $ \theta $, is:
\(P(y_n | \pi, \theta) = \sum_{k=1}^K \pi_k f(y_n | \theta_k)\)</li>
    </ul>
  </li>
  <li><strong>Posterior</strong>:
    <ul>
      <li>Using Bayes’ theorem, the posterior distribution updates our beliefs about the parameters based on the observed data:
\(P(\pi, \theta | y) \propto P(y | \pi, \theta) P(\pi) P(\theta)\)</li>
    </ul>
  </li>
</ol>

<h2 id="inference-in-mixture-models-with-pymc">Inference in Mixture Models with PyMC</h2>

<p>Now let’s simulate a mixture model and infer the parameters using MCMC. First, let’s create an interactive way to generate data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">matplotlib.colors</span> <span class="k">as</span> <span class="n">mcolors</span>
<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="kn">import</span> <span class="n">ipywidgets</span> <span class="k">as</span> <span class="n">widgets</span>
<span class="kn">from</span> <span class="n">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>

<span class="k">def</span> <span class="nf">get_color_gradient</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">color1</span><span class="o">=</span><span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">,</span> <span class="n">color2</span><span class="o">=</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">cmap</span> <span class="o">=</span> <span class="n">mcolors</span><span class="p">.</span><span class="n">LinearSegmentedColormap</span><span class="p">.</span><span class="nf">from_list</span><span class="p">(</span><span class="sh">"</span><span class="s">gradient</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="n">color1</span><span class="p">,</span> <span class="n">color2</span><span class="p">])</span>
    <span class="k">return</span> <span class="p">[</span><span class="nf">cmap</span><span class="p">(</span><span class="n">i</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>

<span class="k">def</span> <span class="nf">plot_mixture</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">imeans</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">iweights</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">show_comps</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

    <span class="c1"># Plot the mixture
</span>    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">patches</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">26</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="sh">'</span><span class="s">black</span><span class="sh">'</span><span class="p">,</span> 
                           <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">skyblue</span><span class="sh">"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Data</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">patch</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">patches</span><span class="p">):</span>
        <span class="n">patch</span><span class="p">.</span><span class="nf">set_facecolor</span><span class="p">(</span><span class="nf">get_color_gradient</span><span class="p">(</span><span class="mi">26</span><span class="p">)[</span><span class="n">i</span><span class="p">])</span>

    <span class="k">if</span> <span class="n">imeans</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">iweights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

        <span class="c1"># Plot the mixture uncertainty
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">iweights</span><span class="p">)):</span>
            <span class="n">pdf</span> <span class="o">=</span> <span class="p">(</span><span class="n">iweights</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">norm</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">imeans</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> 
                   <span class="n">iweights</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">norm</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">imeans</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pdf</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">orange</span><span class="sh">"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.003</span><span class="p">)</span>

        <span class="c1"># Plot the mixture mean
</span>        <span class="n">mixture_density</span> <span class="o">=</span> <span class="p">(</span><span class="n">iweights</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">norm</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">imeans</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span>
                          <span class="n">iweights</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">norm</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">imeans</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mixture_density</span><span class="p">,</span> <span class="sh">"</span><span class="s">k--</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Mixture Mean</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># Plot the components
</span>    <span class="k">if</span> <span class="n">show_comps</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">weight</span><span class="p">,</span> <span class="n">mean</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">iweights</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">imeans</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)):</span>
            <span class="n">component_density</span> <span class="o">=</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">norm</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">component_density</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">--</span><span class="sh">"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> 
                   <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">Component: μ=</span><span class="si">{</span><span class="n">mean</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">Mixture of Gaussians</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">y</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Density</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="c1"># Interactive widget for generating mixture data
</span><span class="n">a</span> <span class="o">=</span> <span class="n">widgets</span><span class="p">.</span><span class="nc">FloatSlider</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">mean_a</span> <span class="o">=</span> <span class="n">widgets</span><span class="p">.</span><span class="nc">FloatSlider</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="mi">5</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">mean_b</span> <span class="o">=</span> <span class="n">widgets</span><span class="p">.</span><span class="nc">FloatSlider</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="mi">5</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">update_plot</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">mean_1</span><span class="p">,</span> <span class="n">mean_2</span><span class="p">):</span>
    <span class="n">components</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="p">[</span><span class="n">weight</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">weight</span><span class="p">])</span>
    <span class="k">global</span> <span class="n">y</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">([</span><span class="n">mean_1</span><span class="p">,</span> <span class="n">mean_2</span><span class="p">][</span><span class="n">k</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">components</span><span class="p">])</span>
    <span class="nf">plot_mixture</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="n">w</span> <span class="o">=</span> <span class="n">widgets</span><span class="p">.</span><span class="nf">interactive</span><span class="p">(</span><span class="n">update_plot</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">a</span><span class="p">,</span> <span class="n">mean_1</span><span class="o">=</span><span class="n">mean_a</span><span class="p">,</span> <span class="n">mean_2</span><span class="o">=</span><span class="n">mean_b</span><span class="p">)</span>
<span class="nf">display</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</code></pre></div></div>

<p>Now let’s fit a Bayesian mixture model to this data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pymc</span> <span class="k">as</span> <span class="n">pm</span>

<span class="c1"># Mixture of Normal Components
</span><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="c1"># Priors
</span>    <span class="n">w</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Dirichlet</span><span class="p">(</span><span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>  <span class="c1"># 2 mixture weights
</span>    <span class="n">mu1</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">"</span><span class="s">mu1</span><span class="sh">"</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">mu2</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">"</span><span class="s">mu2</span><span class="sh">"</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">components</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">.</span><span class="nf">dist</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">mu1</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">.</span><span class="nf">dist</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">mu2</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">]</span>

    <span class="n">like</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Mixture</span><span class="p">(</span><span class="sh">"</span><span class="s">like</span><span class="sh">"</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="n">w</span><span class="p">,</span> <span class="n">comp_dists</span><span class="o">=</span><span class="n">components</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

    <span class="c1"># Sample
</span>    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>Let’s examine our results:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">arviz</span> <span class="k">as</span> <span class="n">az</span>

<span class="c1"># Extract posteriors
</span><span class="n">imeans</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">.</span><span class="n">mu1</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">.</span><span class="n">mu2</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">values</span><span class="p">]).</span><span class="n">T</span>
<span class="n">iweights</span> <span class="o">=</span> <span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">.</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">values</span>

<span class="nf">plot_mixture</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">imeans</span><span class="p">,</span> <span class="n">iweights</span><span class="p">,</span> <span class="n">show_comps</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="nf">plot_posterior</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">round_to</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">ax</span><span class="p">.</span><span class="nf">flatten</span><span class="p">():</span> 
    <span class="n">a</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Density</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="understanding-the-results-1">Understanding the Results</h2>

<p><strong>Notes:</strong></p>

<ol>
  <li><strong>Mixture Fit Results</strong>:
    <ul>
      <li>The observed histogram shows two distinct peaks, which are well-captured by the <strong>Mixture of Gaussians</strong>.</li>
      <li>The dashed black line represents the <strong>mixture mean</strong>, which combines the contributions of both components.</li>
    </ul>
  </li>
  <li><strong>Posterior Summaries</strong>:
    <ul>
      <li>For $ \mu_1 $ (mean of the first component), the posterior mean is approximately <strong>0.03</strong>, indicating the first component centers near zero.</li>
      <li>For $ \mu_2 $ (mean of the second component), the posterior mean is approximately <strong>4.1</strong>, reflecting the second component is centered around 4.</li>
      <li>The weights $ w_0 $ and $ w_1 $ (mixing proportions) show posterior means of <strong>0.7</strong> and <strong>0.3</strong>, respectively, indicating the first component contributes 70% and the second contributes 30% to the mixture.</li>
    </ul>
  </li>
  <li><strong>Uncertainty in Estimation</strong>:
    <ul>
      <li>The credible intervals (94% HDI) for all parameters are narrow, showing high confidence in the estimates for means ($ \mu_1, \mu_2 $) and weights ($ w_0, w_1 $).</li>
    </ul>
  </li>
</ol>

<p><strong>Quiz</strong>: Mixture Peaks - From the plot, why does the first peak appear taller than the second? How do the weights influence this observation?</p>

<h2 id="key-insights-about-mixture-models">Key Insights About Mixture Models</h2>

<ol>
  <li><strong>Hidden Structure</strong>: Mixture models help us discover hidden subgroups in our data</li>
  <li><strong>Automatic Clustering</strong>: The model automatically assigns data points to components</li>
  <li><strong>Flexible Modeling</strong>: We can model complex, multimodal distributions</li>
  <li><strong>Uncertainty in Assignment</strong>: Unlike hard clustering, we get probabilities of membership</li>
</ol>

<h1 id="part-4-markov-models-and-temporal-dependencies">Part 4: Markov Models and Temporal Dependencies</h1>

<p>Imagine our generative process has evolved again. Instead of sampling directly from a single bag or even from multiple nested bags (as in the mixture model), now we have a <strong>sequence</strong> of bags. The bag we sample from at any given step <strong>depends on which bag we chose in the previous step</strong>.</p>

<p>This situation introduces the concept of <strong>dependencies in time or sequence</strong>, where the current state depends on the previous one. Such a process can be described using <strong>Markov Models</strong>.</p>

<h2 id="what-is-a-markov-process">What is a Markov Process?</h2>

<p>A <strong>Markov Process</strong> models systems that evolve over time, where:</p>
<ol>
  <li>The system can be in one of several <strong>states</strong>.</li>
  <li>The probability of transitioning to a new state depends only on the <strong>current state</strong> and not on the sequence of states before it. This is the <strong>Markov Property</strong>:
\(P(z_n | z_{n-1}, z_{n-2}, \dots) = P(z_n | z_{n-1})\)</li>
</ol>

<p>In our example, the system moves from one bag to another over time. The bag you pick your ball from at step $ n $ depends only on the bag you chose at step $ n-1 $.</p>

<h2 id="introducing-hidden-markov-models-hmms">Introducing Hidden Markov Models (HMMs)</h2>

<p><em>[Image Caption: A sequence diagram showing three bags connected by arrows, with each bag producing colored balls. The bags represent hidden states, and the balls represent observations]</em></p>

<p>A <strong>Hidden Markov Model (HMM)</strong> extends a Markov Process by introducing <strong>observations</strong>. You no longer see the actual bag (state), but only the <strong>ball</strong> drawn from it (observation).</p>

<p>Key components of an HMM:</p>
<ol>
  <li><strong>Hidden States ($ z_n $)</strong>:
    <ul>
      <li>Represent which “hidden bag” is active at each time step $ n $.</li>
      <li>Transition between states is governed by the <strong>transition probabilities</strong>.</li>
    </ul>
  </li>
  <li><strong>Observations ($ y_n $)</strong>:
    <ul>
      <li>Drawn from a Gaussian distribution parameterized by the <strong>mean</strong> ($ \mu_k $) and <strong>variance</strong> ($ \sigma_k^2 $) of the active state:
\(y_n \sim \mathcal{N}(\mu_{z_n}, \sigma_{z_n}^2)\)</li>
    </ul>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>**Transition Probabilities ($ P(z_n</td>
          <td>z_{n-1}) $)**:</td>
        </tr>
      </tbody>
    </table>
    <ul>
      <li>Govern how likely it is to move from one state to another.</li>
      <li>Captures the sequential dependencies in the generative process.</li>
    </ul>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>**Emission Probabilities ($ P(y_n</td>
          <td>z_n) $)**:</td>
        </tr>
      </tbody>
    </table>
    <ul>
      <li>Describe the distribution of balls in each bag.</li>
      <li>E.g., a “red-dominant” bag emits more red balls, but it can still emit blue ones occasionally.</li>
    </ul>
  </li>
</ol>

<h2 id="generative-process-for-an-hmm">Generative Process for an HMM</h2>

<p>The generative process of an HMM can be described as follows:</p>

<ol>
  <li><strong>Transition</strong>:
    <ul>
      <li>At time $ n $, transition to a new state $ z_n $ based on the current state $ z_{n-1} $:
\(z_n \sim \text{Categorical}(\pi_{z_{n-1}})\)</li>
    </ul>
  </li>
  <li><strong>Emission</strong>:
    <ul>
      <li>Once in state $ z_n $, generate an observation $ y_n $ from the Gaussian distribution of that state:
\(y_n \sim \mathcal{N}(\mu_{z_n}, \sigma_{z_n}^2)\)</li>
    </ul>
  </li>
</ol>

<h3 id="joint-and-marginal-distributions-1">Joint and Marginal Distributions</h3>

<ol>
  <li>
    <p><strong>Joint Probability of Observations and States</strong>:
\(P(y, z) = P(z_1) \prod_{n=2}^N P(z_n | z_{n-1}) P(y_n | z_n)\)</p>
  </li>
  <li>
    <p><strong>Marginal Probability of Observations</strong>:</p>
    <ul>
      <li>By summing (marginalizing) over all possible hidden states:
\(P(y) = \sum_{z_1, z_2, \dots, z_N} P(y, z)\)</li>
    </ul>
  </li>
</ol>

<p>You can also imagine this as a dynamic mixture model:</p>

<p><em>[Image Caption: A comparison diagram showing static mixture model vs dynamic mixture model (HMM), highlighting how the mixture weights change over time in HMMs]</em></p>

<h2 id="probabilistic-graphical-model-1">Probabilistic Graphical Model</h2>

<p>Below is the PGM illustrating the HMM. It shows the hierarchical dependencies:</p>

<p><em>[Image Caption: A directed graphical model showing the temporal chain z₁ → z₂ → z₃ → … → zₙ, with each zᵢ pointing down to its corresponding observation yᵢ, and μₖ parameters influencing the emissions]</em></p>

<ul>
  <li>$ z_n $: Hidden state at time $ n $, governing the emission and the next state (the bag)</li>
  <li>$ \mu_k$: Parameters of the Gaussian distribution for each hidden state.</li>
  <li>$ y_n $: Observed ball drawn at time $ n $.</li>
  <li>The arrows show how $ z_{n-1} $ influences $ z_n $, and $ z_n $ determines $ y_n $.</li>
</ul>

<h2 id="expectation-maximization-the-frequentist-approach">Expectation-Maximization: The Frequentist Approach</h2>

<p>The <strong>Expectation-Maximization (EM)</strong> algorithm is a frequentist approach to solving Hidden Markov Models. Unlike full Bayesian inference, EM provides <strong>point estimates</strong> of the parameters by maximizing the likelihood of the observed data.</p>

<p>Key differences from Bayesian approach:</p>
<ul>
  <li>We won’t have estimates for the <strong>posterior distribution</strong> of the parameters or uncertainty (credible intervals).</li>
  <li>EM gives a single “best guess” for the parameters (maximum likelihood estimates).</li>
  <li>EM usually does not use priors on the parameters (but we can add them if we want!).</li>
  <li>EM is computationally faster and simpler since it avoids sampling (e.g., MCMC).</li>
</ul>

<h3 id="how-em-solves-hmms">How EM Solves HMMs</h3>

<p>Using <strong>EM</strong>, solving an HMM involves:</p>
<ol>
  <li><strong>E-Step</strong>: Infer the hidden states based on the current parameter estimates.</li>
  <li><strong>M-Step</strong>: Update the parameters to maximize the likelihood given the inferred states.</li>
  <li><strong>Repeat</strong> until the parameters converge.</li>
</ol>

<h2 id="example-solving-gaussian-hmm-with-em">Example: Solving Gaussian HMM with EM</h2>

<p>Let’s implement and solve an HMM using the Dynamax library. First, let’s define our true parameters:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>
<span class="kn">import</span> <span class="n">jax.random</span> <span class="k">as</span> <span class="n">jr</span>
<span class="kn">from</span> <span class="n">jax</span> <span class="kn">import</span> <span class="n">vmap</span>
<span class="kn">from</span> <span class="n">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">matplotlib.colors</span> <span class="k">as</span> <span class="n">mcolors</span>
<span class="kn">from</span> <span class="n">dynamax.hidden_markov_model</span> <span class="kn">import</span> <span class="n">GaussianHMM</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="c1"># True parameters of the generative process
</span><span class="n">true_num_states</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">emission_dim</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># Specify parameters of the HMM
</span><span class="n">initial_probs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="mi">3</span>

<span class="n">transition_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]</span>
<span class="p">])</span>

<span class="n">emission_means</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">3.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.0</span><span class="p">]])</span>
<span class="n">emission_covs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[[</span><span class="mf">0.7</span><span class="p">]],</span> <span class="p">[[</span><span class="mf">0.7</span><span class="p">]],</span> <span class="p">[[</span><span class="mf">0.7</span><span class="p">]]])</span>
</code></pre></div></div>

<p>Let’s visualize the distributions of each state in the HMM:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_color_gradient</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">color1</span><span class="o">=</span><span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">,</span> <span class="n">color2</span><span class="o">=</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">cmap</span> <span class="o">=</span> <span class="n">mcolors</span><span class="p">.</span><span class="n">LinearSegmentedColormap</span><span class="p">.</span><span class="nf">from_list</span><span class="p">(</span><span class="sh">"</span><span class="s">gradient</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="n">color1</span><span class="p">,</span> <span class="n">color2</span><span class="p">])</span>
    <span class="k">return</span> <span class="p">[</span><span class="nf">cmap</span><span class="p">(</span><span class="n">i</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>

<span class="k">def</span> <span class="nf">plot_mixture</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    
    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">patches</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">26</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="sh">'</span><span class="s">black</span><span class="sh">'</span><span class="p">,</span> 
                           <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">skyblue</span><span class="sh">"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Data</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">patch</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">patches</span><span class="p">):</span>
        <span class="n">patch</span><span class="p">.</span><span class="nf">set_facecolor</span><span class="p">(</span><span class="nf">get_color_gradient</span><span class="p">(</span><span class="mi">26</span><span class="p">)[</span><span class="n">i</span><span class="p">])</span>
    
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">State Distribution</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">y</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Density</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Visualize each state's distribution
</span><span class="n">figs</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">true_num_states</span><span class="p">):</span>
    <span class="n">components</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">transition_matrix</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">emission_means</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">emission_covs</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span> 
                  <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">components</span><span class="p">])</span>
    <span class="nf">plot_mixture</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">k</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">State </span><span class="si">{</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s"> Distribution</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Notice how each state has a certain preference for the color of the balls.</strong> These can be considered as state definitions! Notice how each state has got a certain preference for the color of the balls.</p>

<p>Now let’s generate some data from this synthetic process and try to solve it:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">num_train_batches</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">num_test_batches</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">num_timesteps</span> <span class="o">=</span> <span class="mi">200</span>

<span class="c1"># Initialize the HMM
</span><span class="n">g_hmm</span> <span class="o">=</span> <span class="nc">GaussianHMM</span><span class="p">(</span><span class="n">true_num_states</span><span class="p">,</span> <span class="n">emission_dim</span><span class="p">)</span>

<span class="c1"># Convert to JAX arrays
</span><span class="n">transition_matrix</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">transition_matrix</span><span class="p">)</span>
<span class="n">emission_means</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">emission_means</span><span class="p">)</span>
<span class="n">emission_covs</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">emission_covs</span><span class="p">)</span>

<span class="n">true_params</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">g_hmm</span><span class="p">.</span><span class="nf">initialize</span><span class="p">(</span>
    <span class="n">initial_probs</span><span class="o">=</span><span class="n">initial_probs</span><span class="p">,</span>
    <span class="n">transition_matrix</span><span class="o">=</span><span class="n">transition_matrix</span><span class="p">,</span>
    <span class="n">emission_means</span><span class="o">=</span><span class="n">emission_means</span><span class="p">,</span>
    <span class="n">emission_covariances</span><span class="o">=</span><span class="n">emission_covs</span>
<span class="p">)</span>

<span class="c1"># Sample train and test data
</span><span class="n">train_key</span><span class="p">,</span> <span class="n">test_key</span> <span class="o">=</span> <span class="n">jr</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">jr</span><span class="p">.</span><span class="nc">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="nf">vmap</span><span class="p">(</span><span class="nf">partial</span><span class="p">(</span><span class="n">g_hmm</span><span class="p">.</span><span class="n">sample</span><span class="p">,</span> <span class="n">true_params</span><span class="p">,</span> <span class="n">num_timesteps</span><span class="o">=</span><span class="n">num_timesteps</span><span class="p">))</span>
<span class="n">train_true_states</span><span class="p">,</span> <span class="n">train_emissions</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">jr</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">train_key</span><span class="p">,</span> <span class="n">num_train_batches</span><span class="p">))</span>
<span class="n">test_true_states</span><span class="p">,</span> <span class="n">test_emissions</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">jr</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">test_key</span><span class="p">,</span> <span class="n">num_test_batches</span><span class="p">))</span>
</code></pre></div></div>

<p>Let’s visualize the generated data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">col_line</span> <span class="o">=</span> <span class="n">mcolors</span><span class="p">.</span><span class="n">LinearSegmentedColormap</span><span class="p">.</span><span class="nf">from_list</span><span class="p">(</span><span class="sh">"</span><span class="s">gradient</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">])</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">emission</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">train_emissions</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span><span class="n">emission</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="sh">"</span><span class="s">black</span><span class="sh">"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">emission</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">emission</span><span class="p">,</span> 
                   <span class="n">cmap</span><span class="o">=</span><span class="n">col_line</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">"</span><span class="s">+</span><span class="sh">"</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Chain </span><span class="si">{</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Time</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Emission</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="training-the-gaussian-hmm-using-em">Training the Gaussian HMM using EM</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Initialize the parameters using K-Means
</span><span class="n">key</span> <span class="o">=</span> <span class="n">jr</span><span class="p">.</span><span class="nc">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">t_hmm</span> <span class="o">=</span> <span class="nc">GaussianHMM</span><span class="p">(</span><span class="n">num_states</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">emission_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">transition_matrix_stickiness</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>
<span class="n">params</span><span class="p">,</span> <span class="n">props</span> <span class="o">=</span> <span class="n">t_hmm</span><span class="p">.</span><span class="nf">initialize</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="n">key</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="sh">"</span><span class="s">kmeans</span><span class="sh">"</span><span class="p">,</span> <span class="n">emissions</span><span class="o">=</span><span class="n">train_emissions</span><span class="p">)</span>
<span class="n">params</span><span class="p">,</span> <span class="n">lps</span> <span class="o">=</span> <span class="n">t_hmm</span><span class="p">.</span><span class="nf">fit_em</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">props</span><span class="p">,</span> <span class="n">train_emissions</span><span class="p">,</span> <span class="n">num_iters</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="c1"># Plot training progress
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">true_lp</span> <span class="o">=</span> <span class="nf">vmap</span><span class="p">(</span><span class="nf">partial</span><span class="p">(</span><span class="n">g_hmm</span><span class="p">.</span><span class="n">marginal_log_prob</span><span class="p">,</span> <span class="n">params</span><span class="p">))(</span><span class="n">train_emissions</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span>
<span class="n">true_lp</span> <span class="o">+=</span> <span class="n">g_hmm</span><span class="p">.</span><span class="nf">log_prior</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axhline</span><span class="p">(</span><span class="n">true_lp</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">:</span><span class="sh">'</span><span class="p">,</span> 
           <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">True LP = {:.2f}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">true_lp</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">lps</span><span class="p">,</span> <span class="sh">"</span><span class="s">k--</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">EM</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">num epochs</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">log prob</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Training Log Probability</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Quiz</strong>: Why does the EM fit to a better log prob than the Generator? Hint: Think about the number of samples.</p>

<p>Let’s compare the recovered parameters with the true ones:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">dynamax.utils.utils</span> <span class="kn">import</span> <span class="n">find_permutation</span>

<span class="c1"># Extract recovered parameters
</span><span class="n">r_init</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">params</span><span class="p">.</span><span class="n">initial</span><span class="p">.</span><span class="n">probs</span><span class="p">)</span>
<span class="n">r_tr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">params</span><span class="p">.</span><span class="n">transitions</span><span class="p">.</span><span class="n">transition_matrix</span><span class="p">)</span>
<span class="n">r_means</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">params</span><span class="p">.</span><span class="n">emissions</span><span class="p">.</span><span class="n">means</span><span class="p">)</span>
<span class="n">r_covs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">params</span><span class="p">.</span><span class="n">emissions</span><span class="p">.</span><span class="n">covs</span><span class="p">)</span>

<span class="c1"># Compare initial probabilities
</span><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span>
    <span class="sh">"</span><span class="s">State</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="sh">"</span><span class="s">State 1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">State 2</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">State 3</span><span class="sh">"</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Type</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="sh">"</span><span class="s">Original</span><span class="sh">"</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="p">[</span><span class="sh">"</span><span class="s">Recovered</span><span class="sh">"</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Probability</span><span class="sh">"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">initial_probs</span><span class="p">,</span> <span class="n">r_init</span><span class="p">])</span>
<span class="p">})</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">barplot</span><span class="p">(</span>
    <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> 
    <span class="n">x</span><span class="o">=</span><span class="sh">"</span><span class="s">State</span><span class="sh">"</span><span class="p">,</span> 
    <span class="n">y</span><span class="o">=</span><span class="sh">"</span><span class="s">Probability</span><span class="sh">"</span><span class="p">,</span> 
    <span class="n">hue</span><span class="o">=</span><span class="sh">"</span><span class="s">Type</span><span class="sh">"</span><span class="p">,</span> 
    <span class="n">palette</span><span class="o">=</span><span class="nf">get_color_gradient</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">dodge</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">edgecolor</span><span class="o">=</span><span class="sh">"</span><span class="s">black</span><span class="sh">"</span>
<span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Comparison of Original and Recovered Initial Probabilities</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Probability</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">State</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">Initial Probabilities</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Quiz</strong>: The recovered init probs are not exactly the same as the generator probs! Can you think of a reason why? What is going on between state 2 and state 3 init probs? Hint: Take a closer look at our training chains!</p>

<p>Let’s examine the transition matrices:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Compare transition matrices
</span><span class="n">rows</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">Current State 1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Current State 2</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Current State 3</span><span class="sh">"</span><span class="p">]</span>
<span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">Next State 1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Next State 2</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Next State 3</span><span class="sh">"</span><span class="p">]</span>

<span class="n">g_tr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">transition_matrix</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">row_shade</span> <span class="o">=</span> <span class="nf">get_color_gradient</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">rows</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">columns</span><span class="p">):</span>
        <span class="n">data</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span><span class="sh">"</span><span class="s">Column</span><span class="sh">"</span><span class="p">:</span> <span class="n">col</span><span class="p">,</span> <span class="sh">"</span><span class="s">Type</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Original</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Value</span><span class="sh">"</span><span class="p">:</span> <span class="n">g_tr</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]})</span>
        <span class="n">data</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span><span class="sh">"</span><span class="s">Column</span><span class="sh">"</span><span class="p">:</span> <span class="n">col</span><span class="p">,</span> <span class="sh">"</span><span class="s">Type</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Recovered</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Value</span><span class="sh">"</span><span class="p">:</span> <span class="n">r_tr</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]})</span>
    
    <span class="n">plot_data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">sns</span><span class="p">.</span><span class="nf">barplot</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="n">plot_data</span><span class="p">,</span>
        <span class="n">x</span><span class="o">=</span><span class="sh">"</span><span class="s">Column</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">y</span><span class="o">=</span><span class="sh">"</span><span class="s">Value</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">hue</span><span class="o">=</span><span class="sh">"</span><span class="s">Type</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">palette</span><span class="o">=</span><span class="n">row_shade</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="mi">2</span><span class="p">:</span><span class="n">i</span><span class="o">*</span><span class="mi">2</span><span class="o">+</span><span class="mi">2</span><span class="p">],</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
        <span class="n">edgecolor</span><span class="o">=</span><span class="sh">"</span><span class="s">black</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span>
    <span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Transition Probs - </span><span class="si">{</span><span class="n">row</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Probability</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">Transition Probs.</span><span class="sh">"</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Column</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p>Finally, let’s compare the emission distributions:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="kn">import</span> <span class="n">matplotlib.lines</span> <span class="k">as</span> <span class="n">mlines</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">emm_colors</span> <span class="o">=</span> <span class="nf">get_color_gradient</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Plot original emission distributions
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">pdf</span> <span class="o">=</span> <span class="n">norm</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">emission_means</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="n">emission_covs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pdf</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">Original State </span><span class="si">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> 
            <span class="n">color</span><span class="o">=</span><span class="n">emm_colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># Plot recovered emission distributions
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">pdf</span> <span class="o">=</span> <span class="n">norm</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">r_means</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="n">r_covs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pdf</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">Recovered State </span><span class="si">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> 
            <span class="n">color</span><span class="o">=</span><span class="n">emm_colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">--</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Emission Distributions</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Density</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Emission</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Create custom legend handles
</span><span class="n">solid_line</span> <span class="o">=</span> <span class="n">mlines</span><span class="p">.</span><span class="nc">Line2D</span><span class="p">([],</span> <span class="p">[],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">black</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">-</span><span class="sh">'</span><span class="p">,</span> 
                          <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Original</span><span class="sh">'</span><span class="p">)</span>
<span class="n">dashed_line</span> <span class="o">=</span> <span class="n">mlines</span><span class="p">.</span><span class="nc">Line2D</span><span class="p">([],</span> <span class="p">[],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">black</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span> 
                           <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Recovered</span><span class="sh">'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">handles</span><span class="o">=</span><span class="p">[</span><span class="n">solid_line</span><span class="p">,</span> <span class="n">dashed_line</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="sh">"</span><span class="s">upper left</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Notice: the states are permuted!</strong></p>

<p><strong>Quiz</strong>: Can you map the recovered states to generator states (only with critical thinking)?</p>

<h2 id="key-insights-about-hidden-markov-models">Key Insights About Hidden Markov Models</h2>

<ol>
  <li><strong>Temporal Dependencies</strong>: HMMs capture how the current state depends on the previous state</li>
  <li><strong>Hidden Structure</strong>: We observe the outcomes but not the underlying states</li>
  <li><strong>Label Switching</strong>: Parameter recovery can suffer from label switching problems</li>
  <li><strong>Sequential Modeling</strong>: Perfect for time series data with changing regimes</li>
</ol>

<h1 id="part-5-input-dependent-models-and-glms">Part 5: Input-Dependent Models and GLMs</h1>

<p>We’ve journeyed from simple stationary models to complex temporal dependencies. Now we enter the realm of <strong>Dynamic Models</strong> - where external factors influence our generative process. This is where Bayesian modeling becomes incredibly powerful for real-world applications.</p>

<h2 id="linear-regressions-and-the-idea-of-uncertainty">Linear Regressions and the Idea of Uncertainty</h2>

<p>Traditional linear models are based on the equation:
\(y = x \cdot w + c\)</p>

<p>This form represents a <strong>deterministic relationship</strong> between the input $x$ and the output $y$. Every input $x$ maps to a single, exact $y$ without any randomness or uncertainty.</p>

<p>In real-world data, however, outputs are often affected by factors not captured by the model (e.g., measurement errors, hidden variables, or inherent variability). Adding noise $\epsilon \sim \mathcal{N}(0, \sigma^2)$ acknowledges this uncertainty and makes the model more realistic, leading to:</p>

\[y = x \cdot w + c + \epsilon\]

<p>where:</p>
<ul>
  <li>$x$ is the input (features),</li>
  <li>$w$ is the weight vector,</li>
  <li>$c$ is the intercept (bias),</li>
  <li>$\epsilon \sim \mathcal{N}(0, \sigma^2)$ is the Gaussian noise with mean $0$ and variance $\sigma^2$.</li>
</ul>

<p>This can be equivalently <strong>reparameterized</strong> as:</p>

\[y \sim \mathcal{N}(x \cdot w + c, \sigma^2)\]

<p>This directly states that $y$ is drawn from a normal distribution with mean $x \cdot w + c$ and variance $\sigma^2$ (a typical Bayesian representation).</p>

<p>The noisy version better reflects the variability seen in observed data.</p>

<h2 id="example-temperature-dependent-ball-colors">Example: Temperature-Dependent Ball Colors</h2>

<p>What does it mean to be driven by some external factors?</p>

<p>To make the <strong>color of the ball input-dependent</strong>, we can think of <strong>x</strong> as a <strong>temperature scale</strong> (1-dimensional), ranging from a cold “blue” temperature to a warm “red” temperature. The <strong>color of the ball (y)</strong> then depends on this temperature. For instance, at lower temperatures, the balls are more likely to be blue, and as the temperature increases, the likelihood shifts toward red.</p>

<p>Let’s generate some data to illustrate this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">matplotlib.colors</span> <span class="k">as</span> <span class="n">mcolors</span>
<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="c1"># Generate temperature-dependent ball colors
</span><span class="n">w</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">c</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">x</span> <span class="o">*</span> <span class="n">w</span> <span class="o">+</span> <span class="n">c</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>

<span class="c1"># Visualize the data
</span><span class="n">col_line</span> <span class="o">=</span> <span class="n">mcolors</span><span class="p">.</span><span class="n">LinearSegmentedColormap</span><span class="p">.</span><span class="nf">from_list</span><span class="p">(</span><span class="sh">"</span><span class="s">gradient</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">col_line</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">"</span><span class="s">+</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="p">[])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">y (color of ball)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">x (temperature)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Color over temperature</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="model-specification-for-linear-regression-glm">Model Specification for Linear Regression (GLM)</h2>

<p>Let $ x $ represent the temperature, a continuous variable ranging from -10 (coldest) to 10 (hottest).</p>

<p>Modeling the color $y$ of the ball as a linear function of the temperature, we can specify our model as follows:</p>

<p><strong>Priors</strong>: 
\(w \sim \mathcal{N}(0, 1)\) 
\(c \sim \mathcal{N}(0, 1)\)
\(\sigma \sim |\mathcal{N}(0, 1)|\)</p>

<p>Notice how we don’t put any prior on $x$ and only on the parameters - this is the <strong>core of Bayesian modeling</strong>. The prior distributions are initially chosen to be normal with mean $0$ and variance $1$ for simplicity.</p>

<p><strong>Likelihood</strong>: 
\(y \sim \mathcal{N}(x \cdot w + c, \sigma^2)\)</p>

<p><strong>Posterior</strong>:
We will update those priors based on Bayes’ theorem:</p>

\[P(w, c, \sigma | y, x) = \frac{P(y | x, w, c, \sigma) P(w) P(c) P(\sigma)}{P(y)}\]

<p>Where:</p>
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$P(y</td>
          <td>x, w, c, \sigma)$ is the likelihood of observing the data given the parameters.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>$P(w)$, $P(c)$, and $P(\sigma)$ are the prior distributions of the parameters.</li>
  <li>$P(y)$ is the total probability of observing the data.</li>
</ul>

<h2 id="inference-in-linear-regression-with-stan">Inference in Linear Regression with Stan</h2>

<p>Let’s solve this with MCMC using Stan:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tempfile</span>
<span class="kn">from</span> <span class="n">cmdstanpy</span> <span class="kn">import</span> <span class="n">CmdStanModel</span>
<span class="kn">import</span> <span class="n">arviz</span> <span class="k">as</span> <span class="n">az</span>

<span class="c1"># Define the Stan model
</span><span class="n">model_specification</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">
data {
    int&lt;lower=0&gt; N;
    vector[N] x;
    vector[N] y;
}
parameters {
    real w;
    real c;
    real&lt;lower=0&gt; sigma;
}
model {
    // Priors
    w ~ normal(0, 1);
    c ~ normal(0, 1);
    sigma ~ normal(0, 1);
    
    // Likelihood
    y ~ normal( w * x + c, sigma);
}
</span><span class="sh">"""</span>

<span class="c1"># Write the model to a temporary file
</span><span class="k">with</span> <span class="n">tempfile</span><span class="p">.</span><span class="nc">NamedTemporaryFile</span><span class="p">(</span><span class="n">suffix</span><span class="o">=</span><span class="sh">"</span><span class="s">.stan</span><span class="sh">"</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">,</span> <span class="n">delete</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="k">as</span> <span class="n">tmp_file</span><span class="p">:</span>
    <span class="n">tmp_file</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">model_specification</span><span class="p">)</span>
    <span class="n">tmp_stan_path</span> <span class="o">=</span> <span class="n">tmp_file</span><span class="p">.</span><span class="n">name</span>

<span class="c1"># Prepare the data
</span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">N</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">x</span><span class="sh">"</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">y</span><span class="sh">"</span><span class="p">:</span> <span class="n">y</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># Compile and fit the model
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">CmdStanModel</span><span class="p">(</span><span class="n">stan_file</span><span class="o">=</span><span class="n">tmp_stan_path</span><span class="p">)</span>
<span class="n">fit</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">iter_sampling</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Analyze results
</span><span class="n">idata</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="nf">from_cmdstanpy</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span>
<span class="n">az</span><span class="p">.</span><span class="nf">plot_posterior</span><span class="p">(</span><span class="n">idata</span><span class="p">,</span> <span class="n">round_to</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">textsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span><span class="sh">"</span><span class="s">Posterior Distribution of GLM Parameters</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p>Let’s compare our recovered parameters with the true ones:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="k">def</span> <span class="nf">get_color_gradient</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">color1</span><span class="o">=</span><span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">,</span> <span class="n">color2</span><span class="o">=</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">cmap</span> <span class="o">=</span> <span class="n">mcolors</span><span class="p">.</span><span class="n">LinearSegmentedColormap</span><span class="p">.</span><span class="nf">from_list</span><span class="p">(</span><span class="sh">"</span><span class="s">gradient</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="n">color1</span><span class="p">,</span> <span class="n">color2</span><span class="p">])</span>
    <span class="k">return</span> <span class="p">[</span><span class="nf">cmap</span><span class="p">(</span><span class="n">i</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>

<span class="c1"># Compare original vs recovered parameters
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span>
    <span class="sh">"</span><span class="s">Parameter</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">c</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">sigma</span><span class="sh">"</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Type</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="sh">"</span><span class="s">Original</span><span class="sh">"</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="p">[</span><span class="sh">"</span><span class="s">Recovered</span><span class="sh">"</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Value</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span>
        <span class="n">w</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> 
        <span class="n">idata</span><span class="p">.</span><span class="n">posterior</span><span class="p">.</span><span class="n">w</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="nf">flatten</span><span class="p">().</span><span class="nf">mean</span><span class="p">(),</span> 
        <span class="n">idata</span><span class="p">.</span><span class="n">posterior</span><span class="p">.</span><span class="n">c</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="nf">flatten</span><span class="p">().</span><span class="nf">mean</span><span class="p">(),</span> 
        <span class="n">idata</span><span class="p">.</span><span class="n">posterior</span><span class="p">.</span><span class="n">sigma</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="nf">flatten</span><span class="p">().</span><span class="nf">mean</span><span class="p">()</span>
    <span class="p">]</span>
<span class="p">})</span>

<span class="n">sns</span><span class="p">.</span><span class="nf">barplot</span><span class="p">(</span>
    <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> 
    <span class="n">x</span><span class="o">=</span><span class="sh">"</span><span class="s">Parameter</span><span class="sh">"</span><span class="p">,</span> 
    <span class="n">y</span><span class="o">=</span><span class="sh">"</span><span class="s">Value</span><span class="sh">"</span><span class="p">,</span> 
    <span class="n">hue</span><span class="o">=</span><span class="sh">"</span><span class="s">Type</span><span class="sh">"</span><span class="p">,</span> 
    <span class="n">palette</span><span class="o">=</span><span class="nf">get_color_gradient</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">dodge</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">edgecolor</span><span class="o">=</span><span class="sh">"</span><span class="s">black</span><span class="sh">"</span>
<span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Original vs Recovered GLM Parameters</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p>Now let’s visualize the fit with uncertainty:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Visualize the fit with uncertainty
</span><span class="n">col_line</span> <span class="o">=</span> <span class="n">mcolors</span><span class="p">.</span><span class="n">LinearSegmentedColormap</span><span class="p">.</span><span class="nf">from_list</span><span class="p">(</span><span class="sh">"</span><span class="s">gradient</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">col_line</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">"</span><span class="s">+</span><span class="sh">"</span><span class="p">)</span>

<span class="n">p_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="c1"># Plot the uncertainty in the fit
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">w_sample</span> <span class="o">=</span> <span class="n">idata</span><span class="p">.</span><span class="n">posterior</span><span class="p">.</span><span class="n">w</span><span class="p">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
    <span class="n">c_sample</span> <span class="o">=</span> <span class="n">idata</span><span class="p">.</span><span class="n">posterior</span><span class="p">.</span><span class="n">c</span><span class="p">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
    <span class="n">u_y</span> <span class="o">=</span> <span class="n">w_sample</span> <span class="o">*</span> <span class="n">p_x</span> <span class="o">+</span> <span class="n">c_sample</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">p_x</span><span class="p">,</span> <span class="n">u_y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">orange</span><span class="sh">"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># Plot the mean posterior predictive
</span><span class="n">p_y</span> <span class="o">=</span> <span class="n">idata</span><span class="p">.</span><span class="n">posterior</span><span class="p">.</span><span class="n">w</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="nf">flatten</span><span class="p">().</span><span class="nf">mean</span><span class="p">()</span> <span class="o">*</span> <span class="n">p_x</span> <span class="o">+</span> <span class="n">idata</span><span class="p">.</span><span class="n">posterior</span><span class="p">.</span><span class="n">c</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="nf">flatten</span><span class="p">().</span><span class="nf">mean</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">p_x</span><span class="p">,</span> <span class="n">p_y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">black</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">--</span><span class="sh">"</span><span class="p">,</span> 
         <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Mean Post. Predictive</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="p">[])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">y (color of ball)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">x (temperature)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">GLM Fit and Uncertainty in Fit</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p>The orange cloud shows the uncertainty in our linear relationship - this is one of the key advantages of Bayesian GLMs. We don’t just get a single line; we get a full distribution of possible relationships.</p>

<h2 id="input-dependent-mixtures-multiple-factories">Input-Dependent Mixtures: Multiple Factories</h2>

<p><strong><em>IMPORTANT: PLEASE READ THE EXAMPLE</em></strong></p>

<p>Imagine there are <strong>two factories</strong>, each producing balls with specific weight-color patterns:</p>

<ol>
  <li><strong>Factory 1</strong>: Produces balls where heavier balls tend to be redder. The color of the ball increases linearly with its weight but with some variability.</li>
  <li><strong>Factory 2</strong>: Produces balls where heavier balls tend to be bluer. The relationship between weight and color is different and noisier than Factory 1.</li>
</ol>

<p>However, when you receive a shipment of balls, you don’t know which factory produced each ball (this is the <strong>hidden state</strong>). All you see are the weights ($ x $) and colors ($ y $) of the balls.</p>

<p><em>[Image Caption: Two factory diagrams side by side - Factory 1 showing heavier balls getting redder, Factory 2 showing heavier balls getting bluer, with arrows indicating the linear relationships]</em></p>

<h3 id="steps-explaining-the-synthetic-data-generation">Steps explaining the synthetic data generation:</h3>

<ul>
  <li><strong>Step 1</strong>: Randomly assign balls to factories.
    <ul>
      <li>Factory 1 (70% of the time).</li>
      <li>Factory 2 (30% of the time).</li>
    </ul>
  </li>
  <li>
    <p><strong>Step 2</strong>: For each ball, generate a weight ($ x $) uniformly between 1 and 10:
\(x \sim \text{Uniform}(1, 10)\)</p>
  </li>
  <li><strong>Step 3</strong>: Use the corresponding factory’s GLM to generate the ball’s color ($ y $):
    <ul>
      <li>If the ball comes from Factory 1: $ y \sim 0.8 \cdot x + 1.5 + \epsilon $</li>
      <li>If the ball comes from Factory 2: $ y \sim -0.5 \cdot x - 2.0 + \epsilon $</li>
      <li>Where $ \epsilon $ is small Gaussian noise to introduce variability.</li>
    </ul>
  </li>
</ul>

<p>Let’s generate this data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Factory parameters
</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span>
<span class="n">c1</span><span class="p">,</span> <span class="n">c2</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span>
<span class="n">sigma1</span><span class="p">,</span> <span class="n">sigma2</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span>

<span class="c1"># Generate data from each factory
</span><span class="n">x_1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">x_2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">700</span><span class="p">)</span>
<span class="n">y_1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">w1</span> <span class="o">*</span> <span class="n">x_1</span> <span class="o">+</span> <span class="n">c1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma1</span><span class="p">)</span>
<span class="n">y_2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">w2</span> <span class="o">*</span> <span class="n">x_2</span> <span class="o">+</span> <span class="n">c2</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma2</span><span class="p">)</span>

<span class="c1"># Visualize the individual factories
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="n">cmap1</span> <span class="o">=</span> <span class="n">mcolors</span><span class="p">.</span><span class="n">LinearSegmentedColormap</span><span class="p">.</span><span class="nf">from_list</span><span class="p">(</span><span class="sh">"</span><span class="s">gradient</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">])</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x_1</span><span class="p">,</span> <span class="n">y_1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">y_1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">"</span><span class="s">+</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="n">ticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">y (color of ball)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_yticks</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">x (weight)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">From Factory 1</span><span class="sh">"</span><span class="p">)</span>

<span class="n">cmap2</span> <span class="o">=</span> <span class="n">mcolors</span><span class="p">.</span><span class="n">LinearSegmentedColormap</span><span class="p">.</span><span class="nf">from_list</span><span class="p">(</span><span class="sh">"</span><span class="s">gradient</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">])</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x_2</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">y_2</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mi">1</span><span class="o">-</span><span class="n">y_2</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">"</span><span class="s">+</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="n">ticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">y (color of ball)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_yticks</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">x (weight)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">From Factory 2</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span><span class="sh">"</span><span class="s">Truth, Hidden. For understanding only. (notice the colorbars)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="c1"># What you actually observe
</span><span class="n">cmap</span> <span class="o">=</span> <span class="n">mcolors</span><span class="p">.</span><span class="n">LinearSegmentedColormap</span><span class="p">.</span><span class="nf">from_list</span><span class="p">(</span><span class="sh">"</span><span class="s">gradient</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">])</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">tot_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">x_1</span><span class="p">,</span> <span class="n">x_2</span><span class="p">])</span>
<span class="n">tot_y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">y_1</span><span class="p">,</span> <span class="n">y_2</span><span class="p">])</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">tot_x</span><span class="p">,</span> <span class="n">tot_y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">tot_y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">"</span><span class="s">+</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">y (color of ball)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">x (weight)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">What you can Observe</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="mixture-of-glm-specification">Mixture of GLM Specification</h2>

<p><strong>Priors</strong>:</p>
<ul>
  <li><strong>Mixing Weights</strong>:
\(\pi \sim \text{Dirichlet}(\alpha_1, \alpha_2)\)</li>
  <li><strong>GLM Parameters for Each Component</strong>:
\(w_k \sim \mathcal{N}(0, 1), \quad c_k \sim \mathcal{N}(0, 1), \quad \sigma_k \sim |\mathcal{N}(0, 1)| \quad \text{for } k = 1, 2\)</li>
</ul>

<p><strong>Likelihood</strong>:
For each observed $ y_n $:
\(P(y_n | x_n, z_n = k, w_k, c_k, \sigma_k) = \mathcal{N}(x_n \cdot w_k + c_k, \sigma_k^2)\)</p>

<p>The overall likelihood is a mixture:
\(P(y_n | x_n, \pi, \{w_k, c_k, \sigma_k\}_{k=1}^2) = \sum_{k=1}^2 \pi_k \mathcal{N}(y_n | x_n \cdot w_k + c_k, \sigma_k^2)\)</p>

<p><strong>Posterior</strong>:
Using Bayes’ theorem, the posterior is:
\(P(\pi, \{w_k, c_k, \sigma_k\}_{k=1}^2 | y, x) \propto P(y | x, \pi, \{w_k, c_k, \sigma_k\}_{k=1}^2) P(\pi) \prod_{k=1}^2 P(w_k) P(c_k) P(\sigma_k)\)</p>

<p>Let’s implement this mixture of GLMs in Stan:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define the Stan model for mixture of GLMs
</span><span class="n">model_specification</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">
data {
    int&lt;lower=0&gt; N; // Number of data points
    int&lt;lower=0&gt; K; // Number of components

    vector[N] x;
    vector[N] y;
}
parameters {
    simplex[K] pi; // Mixture weights
    vector[K] w; // Slopes
    vector[K] c; // Intercepts
    vector&lt;lower=0&gt;[K] sigma; // Standard deviations
}
model {
    // Priors
    pi ~ dirichlet(rep_vector(1, K));
    w ~ normal(0, 1);
    c ~ normal(0, 1);
    sigma ~ normal(0, 1);
    
    // Likelihood
    for (n in 1:N) {
        vector[K] log_likelihoods;
        for (k in 1:K) {
            log_likelihoods[k] = log(pi[k]) + normal_lpdf(y[n] | x[n] * w[k] + c[k], sigma[k]);
        }
        target += log_sum_exp(log_likelihoods);
    }
}
</span><span class="sh">"""</span>

<span class="c1"># Fit the model
</span><span class="k">with</span> <span class="n">tempfile</span><span class="p">.</span><span class="nc">NamedTemporaryFile</span><span class="p">(</span><span class="n">suffix</span><span class="o">=</span><span class="sh">"</span><span class="s">.stan</span><span class="sh">"</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">,</span> <span class="n">delete</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="k">as</span> <span class="n">tmp_file</span><span class="p">:</span>
    <span class="n">tmp_file</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">model_specification</span><span class="p">)</span>
    <span class="n">tmp_stan_path</span> <span class="o">=</span> <span class="n">tmp_file</span><span class="p">.</span><span class="n">name</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">N</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">K</span><span class="sh">"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">x</span><span class="sh">"</span><span class="p">:</span> <span class="n">tot_x</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">y</span><span class="sh">"</span><span class="p">:</span> <span class="n">tot_y</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">CmdStanModel</span><span class="p">(</span><span class="n">stan_file</span><span class="o">=</span><span class="n">tmp_stan_path</span><span class="p">)</span>
<span class="n">fit</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">iter_sampling</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">idata</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="nf">from_cmdstanpy</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">idata</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">compact</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Notice the label switching!!</strong> All chains look stable so we will stick with one of them to avoid the switching problem.</p>

<p>Let’s visualize the recovered GLMs:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Extract posterior samples (using first chain to avoid label switching)
</span><span class="n">p_pi</span> <span class="o">=</span> <span class="n">idata</span><span class="p">.</span><span class="n">posterior</span><span class="p">.</span><span class="n">data_vars</span><span class="p">[</span><span class="sh">"</span><span class="s">pi</span><span class="sh">"</span><span class="p">].</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">p_w</span> <span class="o">=</span> <span class="n">idata</span><span class="p">.</span><span class="n">posterior</span><span class="p">.</span><span class="n">data_vars</span><span class="p">[</span><span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">].</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">p_c</span> <span class="o">=</span> <span class="n">idata</span><span class="p">.</span><span class="n">posterior</span><span class="p">.</span><span class="n">data_vars</span><span class="p">[</span><span class="sh">"</span><span class="s">c</span><span class="sh">"</span><span class="p">].</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">p_sigma</span> <span class="o">=</span> <span class="n">idata</span><span class="p">.</span><span class="n">posterior</span><span class="p">.</span><span class="n">data_vars</span><span class="p">[</span><span class="sh">"</span><span class="s">sigma</span><span class="sh">"</span><span class="p">].</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Plot the recovered GLMs onto the data
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">mcolors</span><span class="p">.</span><span class="n">LinearSegmentedColormap</span><span class="p">.</span><span class="nf">from_list</span><span class="p">(</span><span class="sh">"</span><span class="s">gradient</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">])</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">tot_x</span><span class="p">,</span> <span class="n">tot_y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">tot_y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">"</span><span class="s">+</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Plot the uncertainty
</span><span class="n">x_line</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">50</span><span class="p">):</span>  <span class="c1"># Sample every 50th to reduce clutter
</span>        <span class="n">pi</span> <span class="o">=</span> <span class="n">p_pi</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">g</span><span class="p">]</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">p_w</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">g</span><span class="p">]</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">p_c</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">g</span><span class="p">]</span>
        <span class="n">u_y</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x_line</span> <span class="o">+</span> <span class="n">c</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_line</span><span class="p">,</span> <span class="n">u_y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">orange</span><span class="sh">"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>

<span class="c1"># Plot the mean posterior predictive
</span><span class="n">p_y_1</span> <span class="o">=</span> <span class="n">p_w</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_line</span> <span class="o">+</span> <span class="n">p_c</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">p_y_2</span> <span class="o">=</span> <span class="n">p_w</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_line</span> <span class="o">+</span> <span class="n">p_c</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_line</span><span class="p">,</span> <span class="n">p_y_1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">black</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">--</span><span class="sh">"</span><span class="p">,</span> 
         <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Component 1 Mean</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_line</span><span class="p">,</span> <span class="n">p_y_2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">black</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span><span class="p">,</span> 
         <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Component 2 Mean</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">(</span><span class="n">sc</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">y (color of ball)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">x (weight)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Mixture of GLMs: Recovered Factory Relationships</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Quiz</strong>: What would happen if we had 3 factories instead?</p>

<h2 id="key-insights-about-input-dependent-models">Key Insights About Input-Dependent Models</h2>

<ol>
  <li><strong>External Influences</strong>: GLMs allow external factors to influence our distributions</li>
  <li><strong>Regression meets Bayesian</strong>: We get uncertainty quantification in our regression parameters</li>
  <li><strong>Mixture Extensions</strong>: We can have different relationships for different subgroups</li>
  <li><strong>Real-world Relevance</strong>: Most real data has input dependencies!</li>
</ol>

<h1 id="part-6-glm-hmms-and-advanced-sequential-modeling">Part 6: GLM-HMMs and Advanced Sequential Modeling</h1>

<p>Welcome to the final part of our Bayesian modeling journey! We’ve traveled from simple balls in bags to complex mixture models and temporal dependencies. Now we’ll combine everything into the most sophisticated model in our series: <strong>Input-Dependent Markov Models</strong>, also known as <strong>GLM-HMMs</strong> or <strong>Switching Linear Regression</strong>.</p>

<p>This is where Bayesian modeling truly shines for complex, real-world sequential data with changing dynamics.</p>

<h2 id="the-conveyor-belt-factory">The Conveyor Belt Factory</h2>

<h3 id="example-conveyor-belts-in-the-color-factory">Example: Conveyor Belts in the Color Factory</h3>

<p>Imagine a <strong>factory</strong> with <strong>three conveyor belts</strong>, each handling different ball-packing operations:</p>

<ol>
  <li><strong>Belt 1 (Red Balls)</strong>:
    <ul>
      <li>On this belt, the redness of a ball ($ y $) depends positively on its weight ($ x $), so heavier balls are redder.</li>
    </ul>
  </li>
  <li><strong>Belt 2 (Blue Balls)</strong>:
    <ul>
      <li>Here, the redness depends negatively on the ball’s weight, making heavier balls bluer.</li>
    </ul>
  </li>
  <li><strong>Belt 3 (Neutral Balls)</strong>:
    <ul>
      <li>This belt doesn’t correlate much with weight, producing balls of all colors.</li>
    </ul>
  </li>
</ol>

<h3 id="the-markovian-twist">The Markovian Twist</h3>

<p>Unlike the mixture model (where balls came independently from factories), this process is sequential:</p>

<ul>
  <li>The conveyor belts operate in a <strong>Markovian sequence</strong>:
    <ul>
      <li>If the current belt is Belt 1, it’s most likely to switch to Belt 2.</li>
      <li>If the current belt is Belt 2, it tends to switch to Belt 3.</li>
      <li>If the current belt is Belt 3, it’s likely to switch back to Belt 1.</li>
    </ul>
  </li>
  <li>You observe the weight ($ x $) and color ($ y $) of each ball in sequence but cannot directly see the belt ($ z $) it came from.</li>
</ul>

<p><em>[Image Caption: Three conveyor belts arranged in a factory setting, with arrows showing the probabilistic transitions between belts. Each belt shows different weight-color relationships - Belt 1 with positive correlation, Belt 2 with negative correlation, Belt 3 with weak correlation]</em></p>

<h2 id="details-on-the-synthetic-data-generation">Details on the Synthetic Data Generation</h2>

<ol>
  <li><strong>Hidden States ($ z_n $)</strong>:
    <ul>
      <li>The conveyor belt in use at time $ n $ (hidden variable).</li>
      <li>Follows a <strong>Markov process</strong>, where:
\(P(z_n | z_{n-1}) = \text{Transition Probability Matrix}\)</li>
      <li>Example transition matrix:
\(\begin{bmatrix}
0.5 &amp; 0.4 &amp; 0.1 \\ 
0.1 &amp; 0.5 &amp; 0.4 \\ 
0.4 &amp; 0.1 &amp; 0.5
\end{bmatrix}\)</li>
    </ul>
  </li>
  <li><strong>Observed Data ($ x, y $)</strong>:
    <ul>
      <li>$ x $: Weight of the ball ($ x \sim \text{Uniform}(1, 10) $).</li>
      <li>$ y $: Color of the ball, determined by the GLM specific to the belt ($ z_n $).</li>
    </ul>
  </li>
  <li><strong>GLMs for Emission Components</strong>:
    <ul>
      <li>Each conveyor belt uses a GLM to model the relationship between weight ($ x $) and color ($ y $):
        <ul>
          <li><strong>Belt 1 (Red)</strong>: $ y_n \sim \mathcal{N}(w_1 \cdot x_n + c_1, \sigma_1^2) $</li>
          <li><strong>Belt 2 (Blue)</strong>: $ y_n \sim \mathcal{N}(w_2 \cdot x_n + c_2, \sigma_2^2) $</li>
          <li><strong>Belt 3 (Neutral)</strong>: $ y_n \sim \mathcal{N}(w_3 \cdot x_n + c_3, \sigma_3^2) $</li>
        </ul>
      </li>
      <li>Their values:
        <ul>
          <li>Belt 1: $ w_1 = 1.0, c_1 = 2.0, \sigma_1 = 1.0 $</li>
          <li>Belt 2: $ w_2 = -0.8, c_2 = -1.0, \sigma_2 = 1.5 $</li>
          <li>Belt 3: $ w_3 = 0.2, c_3 = 0.0, \sigma_3 = 0.8 $</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Sequence Generation</strong>:
    <ul>
      <li>Start with an initial belt ($ z_1 \sim \pi $, where $ \pi $ is the initial state distribution).</li>
      <li>For each time step $ n $:
        <ol>
          <li>Transition to the next belt ($ z_n $) based on the current belt ($ z_{n-1} $).</li>
          <li>Generate the weight ($ x_n $) from a uniform distribution.</li>
          <li>Generate the color ($ y_n $) using the GLM of the current belt ($ z_n $).</li>
        </ol>
      </li>
    </ul>
  </li>
</ol>

<p>Let’s start by generating and visualizing our data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>
<span class="kn">import</span> <span class="n">jax.random</span> <span class="k">as</span> <span class="n">jr</span>
<span class="kn">from</span> <span class="n">jax</span> <span class="kn">import</span> <span class="n">vmap</span>
<span class="kn">from</span> <span class="n">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">matplotlib.colors</span> <span class="k">as</span> <span class="n">mcolors</span>
<span class="kn">from</span> <span class="n">dynamax.hidden_markov_model</span> <span class="kn">import</span> <span class="n">LinearRegressionHMM</span>
<span class="kn">from</span> <span class="n">dynamax.utils.utils</span> <span class="kn">import</span> <span class="n">find_permutation</span>

<span class="c1"># Generator parameters
</span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span>
<span class="n">bias</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>

<span class="n">trans_mat</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
<span class="p">])</span>

<span class="n">initial_probs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="mi">3</span>

<span class="c1"># First, let's see what happens if we ignore the sequential nature
</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">y_1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">bias</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">y_2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">bias</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">y_3</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">weights</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">bias</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

<span class="n">col_line</span> <span class="o">=</span> <span class="n">mcolors</span><span class="p">.</span><span class="n">LinearSegmentedColormap</span><span class="p">.</span><span class="nf">from_list</span><span class="p">(</span><span class="sh">"</span><span class="s">gradient</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">]),</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">y_1</span><span class="p">,</span> <span class="n">y_2</span><span class="p">,</span> <span class="n">y_3</span><span class="p">]),</span> 
           <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">y_1</span><span class="p">,</span> <span class="n">y_2</span><span class="p">,</span> <span class="n">y_3</span><span class="p">]),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">col_line</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">"</span><span class="s">+</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="p">[])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">y (color of ball)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">x (weight)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">If we did not care for the sequence of ball arrival!</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p>As you can see, if the balls all came in the same bag at once, there would not be any Markovian process. In that case it just boils down to a mixture! Now let’s make these balls come in sequences:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Set up the sequential data generation
</span><span class="n">num_states</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">emission_dim</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">covariate_dim</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">num_timesteps</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># Initialize the HMM
</span><span class="n">hmm</span> <span class="o">=</span> <span class="nc">LinearRegressionHMM</span><span class="p">(</span><span class="n">num_states</span><span class="p">,</span> <span class="n">covariate_dim</span><span class="p">,</span> <span class="n">emission_dim</span><span class="p">)</span>

<span class="n">true_params</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hmm</span><span class="p">.</span><span class="nf">initialize</span><span class="p">(</span>
    <span class="n">jr</span><span class="p">.</span><span class="nc">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
    <span class="n">initial_probs</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">initial_probs</span><span class="p">),</span>
    <span class="n">transition_matrix</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">trans_mat</span><span class="p">),</span>
    <span class="n">emission_weights</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">weights</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
    <span class="n">emission_biases</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">bias</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
    <span class="n">emission_covariances</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">sigma</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="p">)</span>

<span class="c1"># Create time-varying inputs (sinusoidal weight pattern)
</span><span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">jnp</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">num_timesteps</span><span class="p">)</span> <span class="o">/</span> <span class="mi">50</span><span class="p">)</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">+</span> <span class="mi">5</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Sample from the true model
</span><span class="n">true_states</span><span class="p">,</span> <span class="n">emissions</span> <span class="o">=</span> <span class="n">hmm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">true_params</span><span class="p">,</span> <span class="n">jr</span><span class="p">.</span><span class="nc">PRNGKey</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">num_timesteps</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">)</span>

<span class="c1"># Generate multiple batches for training
</span><span class="n">batch_inputs</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_timesteps</span><span class="p">,</span> <span class="n">covariate_dim</span><span class="p">)))</span>
<span class="n">batch_true_states</span><span class="p">,</span> <span class="n">batch_emissions</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">hmm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">true_params</span><span class="p">,</span> <span class="n">jr</span><span class="p">.</span><span class="nc">PRNGKey</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">10</span><span class="p">),</span> <span class="n">num_timesteps</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">batch_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">batch_true_states</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">batch_emissions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="n">batch_true_states</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">batch_true_states</span><span class="p">)</span>
<span class="n">batch_emissions</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">batch_emissions</span><span class="p">)</span>
</code></pre></div></div>

<p>Let’s visualize the sequential data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">col_line</span> <span class="o">=</span> <span class="n">mcolors</span><span class="p">.</span><span class="n">LinearSegmentedColormap</span><span class="p">.</span><span class="nf">from_list</span><span class="p">(</span><span class="sh">"</span><span class="s">gradient</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">num_timesteps</span><span class="p">),</span> <span class="n">emissions</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">emissions</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">col_line</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">'</span><span class="s">o</span><span class="sh">'</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Emission Color</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Time</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">If you were given this, could you tell which belt each point came from?</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="c1"># Plot the inputs, emissions, and true states
</span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">gridspec_kw</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">height_ratios</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]},</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">em_max</span> <span class="o">=</span> <span class="n">emissions</span><span class="p">.</span><span class="nf">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">2</span>
<span class="n">em_min</span> <span class="o">=</span> <span class="n">emissions</span><span class="p">.</span><span class="nf">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">2</span>

<span class="c1"># Plot inputs (weights)
</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="sh">'</span><span class="s">k-</span><span class="sh">'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_timesteps</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Plot emissions with true states as background
</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">imshow</span><span class="p">(</span><span class="n">true_states</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:],</span>
              <span class="n">extent</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_timesteps</span><span class="p">,</span> <span class="n">em_min</span><span class="p">,</span> <span class="n">em_max</span><span class="p">),</span>
              <span class="n">aspect</span><span class="o">=</span><span class="sh">"</span><span class="s">auto</span><span class="sh">"</span><span class="p">,</span>
              <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">Grays</span><span class="sh">"</span><span class="p">,</span>
              <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span><span class="n">emissions</span><span class="p">,</span> <span class="sh">'</span><span class="s">k--</span><span class="sh">'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">num_timesteps</span><span class="p">),</span> <span class="n">emissions</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">emissions</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">col_line</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">'</span><span class="s">o</span><span class="sh">'</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_timesteps</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_ylim</span><span class="p">(</span><span class="n">em_min</span><span class="p">,</span> <span class="n">em_max</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">emissions (color)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Time</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span><span class="sh">"</span><span class="s">True Simulated Data</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Can you tell the belts apart now? 🔎</strong></p>

<h2 id="glm-hmm-specification-switching-linear-regression">GLM-HMM Specification (Switching Linear Regression)</h2>

<p><strong>1. Emissions (GLM for Observed Data)</strong>:</p>
<ul>
  <li>The observed data $ y_t $ at time $ t $ is modeled as a Gaussian distribution:
\(y_t \mid x_t, z_t = k \sim \mathcal{N}(x_t \cdot w_k + c_k, \sigma_k^2)\)
where:
    <ul>
      <li>$ z_t \in {1, 2, \dots, K} $: Discrete latent state (which conveyor belt is active at time $ t $).</li>
      <li>$ x_t $: Input feature (ball weight).</li>
      <li>$ w_k $: Weight for the linear relationship in state $ k $.</li>
      <li>$ c_k $: Bias (intercept) for state $ k $.</li>
      <li>$ \sigma_k^2 $: Variance of the Gaussian noise for state $ k $.</li>
    </ul>
  </li>
</ul>

<p><strong>2. Latent States (Markov Process)</strong>:</p>
<ul>
  <li>The discrete latent states $ z_t $ follow a <strong>Markov process</strong>:
\(z_1 \sim \pi, \quad z_{t+1} \mid z_t \sim P_{z_t}\)
where:
    <ul>
      <li>$ \pi $: Initial state distribution.</li>
      <li>$ P_{z_t} $: Transition probability matrix describing transitions between states.</li>
    </ul>
  </li>
</ul>

<p><strong>Priors on Model Parameters</strong>:
\(\pi \sim \text{Dirichlet}(\alpha_{\pi})\)
\(P_{z_t} \sim \text{Dirichlet}(\alpha_P)\)
\(w_k \sim \mathcal{N}(0, 1), \quad c_k \sim \mathcal{N}(0, 1) \quad \text{for } k = 1, \dots, K\)
\(\sigma_k \sim |\mathcal{N}(0, 1)| \quad \text{for } k = 1, \dots, K\)</p>

<p><strong>Likelihood</strong>:</p>

<p>For each observed $ y_t $:</p>
<ol>
  <li>Conditioned on the hidden state $ z_t $, the likelihood is:
\(P(y_t \mid x_t, z_t = k, w_k, c_k, \sigma_k) = \mathcal{N}(y_t \mid x_t \cdot w_k + c_k, \sigma_k^2)\)</li>
  <li>The full likelihood marginalizes over all possible hidden states:
\(P(y_t \mid x_t, \pi, P, \{w_k, c_k, \sigma_k\}_{k=1}^K) = \sum_{k=1}^K P(z_t = k \mid z_{t-1}, P) \cdot \mathcal{N}(y_t \mid x_t \cdot w_k + c_k, \sigma_k^2)\)</li>
</ol>

<p><strong>Posterior</strong>:</p>

<p>Using Bayes’ theorem, the posterior is:
\(P(\pi, P, \{w_k, c_k, \sigma_k\}_{k=1}^K \mid y_{1:T}, x_{1:T}) \propto P(y_{1:T} \mid x_{1:T}, \pi, P, \{w_k, c_k, \sigma_k\}_{k=1}^K) \cdot P(\pi) \cdot P(P) \cdot \prod_{k=1}^K P(w_k) P(c_k) P(\sigma_k)\)</p>

<h2 id="how-to-solve-this-complex-model-">How to Solve This Complex Model? 😱</h2>

<p>We can solve it the same way we have been solving our models. However, the <strong>Markovian nature</strong> of the model introduces <strong>dependencies</strong> between the observations, making it more complex to solve than the previous models. Making full Bayesian inference computationally <strong>VERY</strong> expensive.</p>

<p>For practical purposes, we’ll use the <strong>Expectation-Maximization (EM)</strong> algorithm with optimizations:</p>

<ol>
  <li><strong>Parameter Estimation</strong>: Estimate parameters by maximizing the marginal likelihood</li>
  <li><strong>Forward-Backward Algorithm</strong>: Efficiently compute state probabilities</li>
  <li><strong>Viterbi Algorithm</strong>: Find the most likely sequence of states</li>
</ol>

<p><strong>Let’s go and train it!</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Initialize and fit the GLM-HMM
</span><span class="n">test_params</span><span class="p">,</span> <span class="n">param_props</span> <span class="o">=</span> <span class="n">hmm</span><span class="p">.</span><span class="nf">initialize</span><span class="p">(</span><span class="n">jr</span><span class="p">.</span><span class="nc">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">))</span>

<span class="c1"># Fit the model using EM
</span><span class="n">test_params</span><span class="p">,</span> <span class="n">lps</span> <span class="o">=</span> <span class="n">hmm</span><span class="p">.</span><span class="nf">fit_em</span><span class="p">(</span><span class="n">test_params</span><span class="p">,</span> <span class="n">param_props</span><span class="p">,</span> <span class="n">batch_emissions</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">batch_inputs</span><span class="p">)</span>

<span class="c1"># Plot training progress
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">lps</span><span class="p">,</span> <span class="sh">"</span><span class="s">k--</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">EM Training</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Epoch</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Log Probability</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">GLM-HMM Training Progress</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p>Now let’s see how well our model recovered the true states:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_states</span><span class="p">(</span><span class="n">true_states</span><span class="p">,</span> <span class="n">most_likely_states</span><span class="p">,</span> <span class="n">emissions</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">Did the HMM get it right?</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">gridspec_kw</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">height_ratios</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]},</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">states</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">([</span><span class="n">true_states</span><span class="p">,</span> <span class="n">most_likely_states</span><span class="p">]):</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">imshow</span><span class="p">(</span><span class="n">states</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:],</span>
                    <span class="n">extent</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_timesteps</span><span class="p">,</span> <span class="n">em_min</span><span class="p">,</span> <span class="n">em_max</span><span class="p">),</span>
                    <span class="n">aspect</span><span class="o">=</span><span class="sh">"</span><span class="s">auto</span><span class="sh">"</span><span class="p">,</span>
                    <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">binary</span><span class="sh">"</span><span class="p">,</span>
                    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span><span class="n">emissions</span><span class="p">,</span> <span class="sh">'</span><span class="s">k-</span><span class="sh">'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">num_timesteps</span><span class="p">),</span> <span class="n">emissions</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">emissions</span><span class="p">,</span> 
                      <span class="n">cmap</span><span class="o">=</span><span class="n">col_line</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">'</span><span class="s">o</span><span class="sh">'</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_timesteps</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">set_ylim</span><span class="p">(</span><span class="n">em_min</span><span class="p">,</span> <span class="n">em_max</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">emissions</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">time</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">true states</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">inferred states</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>

<span class="c1"># Compute the most likely states
</span><span class="n">most_likely_states</span> <span class="o">=</span> <span class="n">hmm</span><span class="p">.</span><span class="nf">most_likely_states</span><span class="p">(</span><span class="n">test_params</span><span class="p">,</span> <span class="n">emissions</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">)</span>
<span class="nf">plot_states</span><span class="p">(</span><span class="n">true_states</span><span class="p">,</span> <span class="n">most_likely_states</span><span class="p">,</span> <span class="n">emissions</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p>It looks pretty bad, doesn’t it? But look closer - this is our old friend <strong>LABEL SWITCHING</strong>! Let’s align them:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">permutation</span> <span class="o">=</span> <span class="nf">find_permutation</span><span class="p">(</span><span class="n">true_states</span><span class="p">,</span> <span class="n">most_likely_states</span><span class="p">)</span>
<span class="n">remapped_states</span> <span class="o">=</span> <span class="n">permutation</span><span class="p">[</span><span class="n">true_states</span><span class="p">]</span>
<span class="nf">plot_states</span><span class="p">(</span><span class="n">remapped_states</span><span class="p">,</span> <span class="n">most_likely_states</span><span class="p">,</span> <span class="n">emissions</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">How about now after alignment?</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p>Much better!</p>

<p><strong>QUIZ</strong>: Why does it get certain time points exceptionally wrong? (Even after label switching, why is it extra difficult?) Hint: Look at the input closely.</p>

<h2 id="parameter-recovery-analysis">Parameter Recovery Analysis</h2>

<p>Let’s examine how well we recovered the original parameters:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="k">def</span> <span class="nf">get_color_gradient</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">color1</span><span class="o">=</span><span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">,</span> <span class="n">color2</span><span class="o">=</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">cmap</span> <span class="o">=</span> <span class="n">mcolors</span><span class="p">.</span><span class="n">LinearSegmentedColormap</span><span class="p">.</span><span class="nf">from_list</span><span class="p">(</span><span class="sh">"</span><span class="s">gradient</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="n">color1</span><span class="p">,</span> <span class="n">color2</span><span class="p">])</span>
    <span class="k">return</span> <span class="p">[</span><span class="nf">cmap</span><span class="p">(</span><span class="n">i</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>

<span class="c1"># Extract recovered parameters
</span><span class="n">t_tr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">test_params</span><span class="p">.</span><span class="n">transitions</span><span class="p">.</span><span class="n">transition_matrix</span><span class="p">)</span>
<span class="n">t_init</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">test_params</span><span class="p">.</span><span class="n">initial</span><span class="p">.</span><span class="n">probs</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">t_weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">test_params</span><span class="p">.</span><span class="n">emissions</span><span class="p">.</span><span class="n">weights</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">t_covs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">test_params</span><span class="p">.</span><span class="n">emissions</span><span class="p">.</span><span class="n">covs</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">t_bias</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">test_params</span><span class="p">.</span><span class="n">emissions</span><span class="p">.</span><span class="n">biases</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Compare transition matrices
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Apply permutation to true parameters for comparison
</span><span class="n">true_trans_aligned</span> <span class="o">=</span> <span class="n">trans_mat</span><span class="p">[</span><span class="n">permutation</span><span class="p">,</span> <span class="p">:][:,</span> <span class="n">permutation</span><span class="p">]</span>

<span class="n">sns</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">true_trans_aligned</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">Greens</span><span class="sh">"</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">True Transition Matrix</span><span class="sh">"</span><span class="p">)</span>

<span class="n">sns</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">t_tr</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">Greens</span><span class="sh">"</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">Recovered Transition Matrix</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="c1"># Compare GLM parameters
</span><span class="k">def</span> <span class="nf">plot_param_comparison</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">r_param</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">ylabel</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.35</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

    <span class="n">states</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">param</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">param</span><span class="p">))</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="nf">get_color_gradient</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="n">ax</span><span class="p">.</span><span class="nf">bar</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">width</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">param</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Generator</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">bar</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">width</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">r_param</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Recovered</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xticks</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="sa">f</span><span class="sh">'</span><span class="s">State </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="sh">'</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">states</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="n">ylabel</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">States</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># Plot each parameter comparison with permutation applied
</span><span class="nf">plot_param_comparison</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="n">permutation</span><span class="p">],</span> <span class="n">t_weights</span><span class="p">,</span> <span class="sh">"</span><span class="s">Weights Comparison</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Weight Value</span><span class="sh">"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="nf">plot_param_comparison</span><span class="p">(</span><span class="n">bias</span><span class="p">[</span><span class="n">permutation</span><span class="p">],</span> <span class="n">t_bias</span><span class="p">,</span> <span class="sh">"</span><span class="s">Bias Comparison</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Bias Value</span><span class="sh">"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="nf">plot_param_comparison</span><span class="p">(</span><span class="n">sigma</span><span class="p">[</span><span class="n">permutation</span><span class="p">],</span> <span class="n">t_covs</span><span class="p">,</span> <span class="sh">"</span><span class="s">Sigma Comparison</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Sigma Value</span><span class="sh">"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="nf">plot_param_comparison</span><span class="p">(</span><span class="n">initial_probs</span><span class="p">[</span><span class="n">permutation</span><span class="p">],</span> <span class="n">t_init</span><span class="p">,</span> <span class="sh">"</span><span class="s">Initial Probabilities Comparison</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Probability</span><span class="sh">"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Considering these are point estimates from only 10 batches, the results are pretty good!</strong></p>

<h2 id="final-thoughts">Final Thoughts</h2>

<p>Bayesian modeling is both an art and a science. The art lies in choosing appropriate priors and model structures that capture the essence of your problem. The science lies in the rigorous mathematical framework that ensures coherent uncertainty quantification and principled inference.</p>

<p><strong>Thank you for attending my Ted Talk! 🙇🎤</strong></p>]]></content><author><name>Rudramani Singha</name></author><category term="thoughts" /><summary type="html"><![CDATA[Welcome to our comprehensive Bayesian modeling tutorial! We’ll start with the fundamentals and work our way up to complex models. Think of this as your journey from understanding basic probability to building sophisticated inference systems.]]></summary></entry><entry><title type="html">Estimates from Behavioral Landmarks</title><link href="http://localhost:4000/posts/sdr/" rel="alternate" type="text/html" title="Estimates from Behavioral Landmarks" /><published>2022-01-13T20:27:20-05:00</published><updated>2022-01-13T20:27:20-05:00</updated><id>http://localhost:4000/posts/sdr</id><content type="html" xml:base="http://localhost:4000/posts/sdr/"><![CDATA[]]></content><author><name>Rudramani Singha</name></author><category term="peer-reviewed" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Time Branching Networks</title><link href="http://localhost:4000/posts/vsd/" rel="alternate" type="text/html" title="Time Branching Networks" /><published>2022-01-12T20:27:20-05:00</published><updated>2022-01-12T20:27:20-05:00</updated><id>http://localhost:4000/posts/vsd</id><content type="html" xml:base="http://localhost:4000/posts/vsd/"><![CDATA[]]></content><author><name>Rudramani Singha</name></author><category term="peer-reviewed" /><summary type="html"><![CDATA[]]></summary></entry></feed>