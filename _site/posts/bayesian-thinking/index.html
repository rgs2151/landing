<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Tutorial on Bayesian Modeling | Rudramani Singha</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Tutorial on Bayesian Modeling" />
<meta name="author" content="Rudramani Singha" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="1. Refresher on Probabilities" />
<meta property="og:description" content="1. Refresher on Probabilities" />
<link rel="canonical" href="http://localhost:4000/~rgs2151/posts/bayesian-thinking/" />
<meta property="og:url" content="http://localhost:4000/~rgs2151/posts/bayesian-thinking/" />
<meta property="og:site_name" content="Rudramani Singha" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-11-23T20:27:20-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Tutorial on Bayesian Modeling" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Rudramani Singha"},"dateModified":"2024-11-23T20:27:20-05:00","datePublished":"2024-11-23T20:27:20-05:00","description":"1. Refresher on Probabilities","headline":"Tutorial on Bayesian Modeling","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/~rgs2151/posts/bayesian-thinking/"},"url":"http://localhost:4000/~rgs2151/posts/bayesian-thinking/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/~rgs2151/assets/main.css">

  <link rel="preconnect" href="https://fonts.googleapis.com" />
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
	<link href="https://fonts.googleapis.com/css2?family=Mulish:ital,wght@0,200..1000;1,200..1000&display=swap" rel="stylesheet" />

  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML">
    MathJax.Hub.Config({
      "HTML-CSS": {
        availableFonts: ["TeX"],
      },
      tex2jax: {
        inlineMath: [['$','$'],["\\(","\\)"]]},
        displayMath: [ ['$$','$$'], ['\[','\]'] ],
      TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "color.js"],
        equationNumbers: {
          autoNumber: "AMS"
        }
      },
      showProcessingMessages: false,
      messageStyle: "none",
      imageFont: null,
      "AssistiveMML": { disabled: true }
    });
</script><script>
if(!(window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1")) {
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-NNNNNNNN-N', 'auto');
  ga('send', 'pageview');
}
</script>
  
</head>
<body>
    
    <main class="page">
      
      <nav class="nav">

    

    <a class="nav__item " aria-label="Home Page" href="/~rgs2151/">
        <svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg">
            <path
                d="M7.07926 0.222253C7.31275 -0.007434 7.6873 -0.007434 7.92079 0.222253L14.6708 6.86227C14.907 7.09465 14.9101 7.47453 14.6778 7.71076C14.4454 7.947 14.0655 7.95012 13.8293 7.71773L13 6.90201V12.5C13 12.7761 12.7762 13 12.5 13H2.50002C2.22388 13 2.00002 12.7761 2.00002 12.5V6.90201L1.17079 7.71773C0.934558 7.95012 0.554672 7.947 0.32229 7.71076C0.0899079 7.47453 0.0930283 7.09465 0.32926 6.86227L7.07926 0.222253ZM7.50002 1.49163L12 5.91831V12H10V8.49999C10 8.22385 9.77617 7.99999 9.50002 7.99999H6.50002C6.22388 7.99999 6.00002 8.22385 6.00002 8.49999V12H3.00002V5.91831L7.50002 1.49163ZM7.00002 12H9.00002V8.99999H7.00002V12Z"
                fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path>
        </svg> </a>
    
    <a class="nav__item " aria-label="Writing Page" href="/~rgs2151/posts/">
        <svg width="15" height="15" viewBox="0 0 15 15" fill="none" xmlns="http://www.w3.org/2000/svg">
            <path
                d="M11.8536 1.14645C11.6583 0.951184 11.3417 0.951184 11.1465 1.14645L3.71455 8.57836C3.62459 8.66832 3.55263 8.77461 3.50251 8.89155L2.04044 12.303C1.9599 12.491 2.00189 12.709 2.14646 12.8536C2.29103 12.9981 2.50905 13.0401 2.69697 12.9596L6.10847 11.4975C6.2254 11.4474 6.3317 11.3754 6.42166 11.2855L13.8536 3.85355C14.0488 3.65829 14.0488 3.34171 13.8536 3.14645L11.8536 1.14645ZM4.42166 9.28547L11.5 2.20711L12.7929 3.5L5.71455 10.5784L4.21924 11.2192L3.78081 10.7808L4.42166 9.28547Z"
                fill="currentColor" fill-rule="evenodd" clip-rule="evenodd"></path>
        </svg></a>
    
    <a class="nav__item" aria-label="Resume" href="/~rgs2151/assets/resume.pdf" target="_blank">
        <svg width="15" height="15" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24"
            stroke-width="1.5" stroke="currentColor">
            <path stroke-linecap="round" stroke-linejoin="round"
                d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z" />
        </svg>
    </a>

    </a>
</nav>

      <header class="page__header">
        <h1 class="page__title">Tutorial on Bayesian Modeling</h1>
        
        <div style="height: 1rem;"></div><div class="postslist__item__date">Nov 23, 2024</div>
        
			</header>

      <div class="page__content">
        <h2 id="1-refresher-on-probabilities">1. Refresher on Probabilities</h2>

<h3 id="11-probability-and-quantifying-variability">1.1 Probability and Quantifying Variability</h3>

<p>Probability is defined as a number between 0 and 1, which describes the likelihood of the occurrence of some particular event in some range of events. 0 means an infinitely unlikely event, and 1 means the certain event. The term ‘event’ is very general, but for us can usually be thought of as one sample data point in an experiment, and the ‘range of events’ would be all the data we would get if we sampled virtually an infinite dataset on the experiment. For now, We are only interested in quantifying the sample variability. <a href="https://geofaculty.uwyo.edu/neil/teaching/Numerical_web/Yaxis_values.pdf">ref</a></p>

<p>Imagine a bag with red and blue balls as such: 6 red balls and 6 blue balls.</p>

<p>If you randomly “sample” one ball (a.k.a. event), the probability of getting a red ball is the fraction of red balls to the total.</p>

\[P(Red)= \frac {\text{Number of Red Balls}​} {\text{Total Number of Balls}}\\\]

<p>Similarly, the probability of getting a blue ball is the fraction of blue balls to the total.
\(P(Blue)= \frac {\text{Number of Blue Balls}​} {\text{Total Number of Balls}}\\\)</p>

<p>And the sum of the probabilities of all possible outcomes would be 1.
\(P(Red) + P(Blue) = 1\)</p>

<p>If you sampled a ball and put it back in the bag, the probability of getting a red ball would be the same on the next draw.<br />
However, if you sampled a ball and didn’t put it back in the bag, the probability of getting a red ball would change for the next draw. (It would be less than previous event)<br />
This is because the sample space has changed (we wont go too much into this for now).</p>

<p><strong>For the rest of this tutorial, we will assume that the generative process that makes these balls can infinitly keep making balls and we can draw as much as we want to discover the true distribution.</strong><br />
If you sampled a ball and put it back in the bag, that is your generative process.<br />
In this manner, if you repeatedly sample from the bag, there is no limit to the number balls you could draw.<br />
With sufficient draws you can be more and more confident about the vraiability in the probability and the true distribution in the generative process.<br />
With a large dataset, the sample probability will converge to the true probability which sould be .5/.5 in this case.</p>

<p><strong><em>Run the code cell below multiple times and see which graph changes most dramatically between runs</em></strong></p>

<h3 id="12-updating-beliefs-with-new-data">1.2 Updating beliefs with new data</h3>

<p>As you intuitively understood from above experiment, as you draw more and more samples, you can be more and more confident about the true distribution of the balls in the bag.<br />
This is the <strong>basis of Bayesian statistics</strong>.</p>

<p>We can formalize this process with Bayes’ theorem:</p>

\[P(\text{Red | Data}) = \frac{P(\text{Data | Red}) P(\text{Red})}{P(\text{Data})}\]

<p>So, basically, as you you aquired more data, you updated your belief about the proportion of red balls in the bag. (which in this case is the same as the probability)</p>

<h3 id="13-discrete-to-continuous-space">1.3 Discrete to Continuous space</h3>

<p><strong>Notice</strong> how you could either get a red ball or either a blue ball. This means your <strong>choices are discrete.</strong></p>

<p>Sometimes, it is possible that the generative process is <strong>continuous</strong>.<br />
Such as, what if the bag had balls in a <strong>spectrum of colors</strong> between red and blue and till now you were only sampling from a specific subset of the dataset. And now for some unknown reason, the generative process has changed and you are now sampling from the whole dataset.\</p>

<p>To discover this new true distribution of the generative process, we make histograms of the samples we draw.<br />
By making the buckets in the histograms thinner and thinner (to infinity), we can get a better idea of the <strong><em>true continuous distribution</em></strong>.</p>

<h2 id="2-bayesian-thinking">2. Bayesian Thinking</h2>

<h3 id="21-probality-interpretation">2.1 Probality Interpretation</h3>

<p>Choosing the right distribution is a question of what fits the data best.<br />
And <strong>when we are aproximating the data, we are fitting a model!</strong></p>

<p>There are 2 schools of thought on this:</p>
<ol>
  <li>
    <p><strong>Frequentist</strong>: They consider the data as fixed (given the experiment) and view the model parameters as unknown but fixed quantities. They use methods like Maximum Likelihood Estimation (MLE) to estimate the parameters that maximize the likelihood of observing the data.</p>
  </li>
  <li>
    <p><strong>Bayesian</strong>: They consider the data as observed, and the <strong>model parameters as random variables with prior beliefs</strong>. They use <strong>Bayes’ theorem to update their prior beliefs</strong> about the parameters into posterior distributions based on the observed data.</p>
  </li>
</ol>

<p><strong>For the purpose of this tutorial, we will focus specifically on the Bayesian approach.</strong></p>

<h3 id="22-bayesian-model-specification">2.2 Bayesian Model Specification</h3>

<p><strong>Bayesian model specification involves 3 components:</strong></p>

<ol>
  <li><strong>Prior</strong>: The prior distribution describes the beliefs about the model parameters before observing the data. It is often assumed to be a normal distribution.</li>
  <li><strong>Likelihood</strong>: The likelihood function describes the probability of observing the data given the model parameters. It is the foundation of the model.</li>
  <li><strong>Posterior</strong>: The posterior distribution describes the updated beliefs about the model parameters after observing the data. It is calculated using Bayes’ theorem.</li>
</ol>

<p>We can update our beliefs about the generative process by using the posterior distribution:
\(Posterior = \frac{Likelihood \times Prior}{Evidence}\)</p>

<p><strong>For our example of the balls in the bag, we can specify the model as follows:</strong></p>

<p><strong>Prior</strong>:</p>

<p>\(\mu \sim \mathcal{N}(0, 1)\) 
\(\sigma \sim |\mathcal{N}(0, 1)|\)</p>

<p><strong>Likelihood</strong>:
\(y \sim \mathcal{N}( \mu, \sigma^2)\)</p>

<p><strong>Posterior</strong>:
\(P(\mu, \sigma | y) = \frac{P(y | \mu, \sigma) P(\mu) P(\sigma)}{P(y)}\)</p>

<p><strong>Where</strong>:</p>
<ul>
  <li>$y$ is the data (balls drawn from the bag).</li>
  <li>$\mu$ is the mean of the distribution.</li>
  <li>$\sigma$ is the standard deviation of the distribution.</li>
  <li>$\mathcal{N}$ is the normal distribution.</li>
</ul>

<h3 id="23-inference">2.3 Inference</h3>

<p>Yayy, now that we have everything, <em>LETS GO AND SOLVE IT!!!!</em><br />
But wait, how do we solve it?</p>

<table>
  <tbody>
    <tr>
      <td>Solving means calculating the posterior distribution $P(\mu, \sigma</td>
      <td>y)$.</td>
    </tr>
  </tbody>
</table>

<ul>
  <li><strong>Analytically</strong>:<br />
We can solve the posterior distribution analytically for simple models such as this.</li>
  <li><strong>Numerically</strong>:<br />
For complex models, it is often <strong>intractable to solve</strong> the posterior distribution analytically. In such cases, we can use numerical methods like <strong>Markov Chain Monte Carlo (MCMC)</strong> or <strong>Variational Inference (VI)</strong> to approximate the posterior distribution.</li>
</ul>

<p><strong>For the purpose of this tutorial, we will use MCMC to approximate the posterior distribution.</strong></p>

<h2 id="2-linear-regressions">2. Linear Regressions</h2>

<h3 id="21-idea-of-uncertainty">2.1 Idea of Uncertainty</h3>

<p>Traditional linear models are based on the equation:
\(y = x \cdot w + c\)
This form represents a <strong>deterministic relationship</strong> between the input $x$ and the output $y$. Every input $x$ maps to a single, exact $y$ without any randomness or uncertainty.</p>

<p>In real-world data, however, outputs are often affected by factors not captured by the model (e.g., measurement errors, hidden variables, or inherent variability). Adding noise $\epsilon \sim \mathcal{N}(0, \sigma^2)$ (for example) acknowledges this uncertainty and makes the model more realistic, leading to:</p>

<p>\(y = x \cdot w + c + \epsilon\)
where:</p>
<ul>
  <li>$x$ is the input (features),</li>
  <li>$w$ is the weight vector,</li>
  <li>$c$ is the intercept (bias),</li>
  <li>$\epsilon \sim \mathcal{N}(0, \sigma^2)$ is the Gaussian noise with mean $0$ and variance $\sigma^2$.</li>
</ul>

<p>This can be equivalently <strong>reparameterized</strong> as:</p>

\[y \sim \mathcal{N}(x \cdot w + c, \sigma^2)\]

<p>This directly states that $y$ is drawn from a normal distribution with mean $x \cdot w + c$ and variance $\sigma^2$. (A typical Bayesian representation)</p>

<p>The noisy version better reflects the variability seen in observed data.</p>

<h3 id="22-model-specification-for-bayesian-linear-regression">2.2 Model Specification for Bayesian Linear Regression</h3>
<p>We can specify our model and its parameters as follows:</p>

<p><strong>Priors</strong>: 
\(w \sim \mathcal{N}(0, 1)\) 
\(c \sim \mathcal{N}(0, 1)\)
\(\sigma \sim |\mathcal{N}(0, 1)|\)
The prior distributions are initially chosen to be normal with mean $0$ and variance $1$ for simplicity.</p>

<p><strong>Likelihood</strong>: 
\(y \sim \mathcal{N}(x \cdot w + c, \sigma^2)\)</p>

<p><strong>Posterior</strong>:<br />
We will update those priors based on Bayes’ theorem (above):</p>

\[P(w, c, \sigma | y, x) = \frac{P(y | x, w, c, \sigma) P(w) P(c) P(\sigma)}{P(y)}\]

<p>Where:</p>
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$P(y</td>
          <td>x, w, c, \sigma)$ is the likelihood of observing the data given the parameters.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>$P(w)$, $P(c)$, and $P(\sigma)$ are the prior distributions of the parameters.</li>
  <li>$P(y)$ is the total probability of observing the data.</li>
</ul>

<!-- 
 -->

      </div>

      
      
      <footer class="footer">
        <div class="footer__cp">© 2024</div>
      </footer>
    
    </main>

  </body>

</html>
