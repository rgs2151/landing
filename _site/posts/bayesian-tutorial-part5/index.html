<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Page title - only the page title, no site title -->
  <title>Bayesian Modeling - Part 5: Input-Dependent Models and GLMs</title>
  
  <!-- Meta description for social sharing -->
  <meta name="description" content="Hey there! I&#39;m a researcher based in New York City. I build probabilistic models and reverse engineer the brain.">
  
  <!-- Author -->
  <meta name="author" content="Rudramani Singha">
  
  <!-- Canonical URL -->
  <link rel="canonical" href="http://localhost:4000/posts/bayesian-tutorial-part5/">
  
  <!-- Open Graph meta tags for social sharing -->
  <meta property="og:title" content="Bayesian Modeling - Part 5: Input-Dependent Models and GLMs">
  <meta property="og:description" content="Hey there! I&#39;m a researcher based in New York City. I build probabilistic models and reverse engineer the brain.">
  <meta property="og:url" content="http://localhost:4000/posts/bayesian-tutorial-part5/">
  <meta property="og:site_name" content="Rudramani Singha">
  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2025-01-19T00:00:00-05:00">
  
  
  <!-- Twitter Card meta tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Bayesian Modeling - Part 5: Input-Dependent Models and GLMs">
  <meta name="twitter:description" content="Hey there! I&#39;m a researcher based in New York City. I build probabilistic models and reverse engineer the brain.">
  
  
  <!-- JSON-LD structured data for rich snippets -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "Bayesian Modeling - Part 5: Input-Dependent Models and GLMs",
    "description": "Hey there! I&#39;m a researcher based in New York City. I build probabilistic models and reverse engineer the brain.",
    "url": "http://localhost:4000/posts/bayesian-tutorial-part5/",
    "datePublished": "2025-01-19T00:00:00-05:00",
    "author": {
      "@type": "Person",
      "name": "Rudramani Singha"
    },
    "publisher": {
      "@type": "Person",
      "name": "Rudramani Singha"
    }
  }
  </script>
  
  <!-- RSS feed -->
  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Rudramani Singha">
  
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" >
  <link rel="stylesheet" href="/assets/css/main.css">

  <link rel="preconnect" href="https://fonts.googleapis.com" />
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
	<link href="https://fonts.googleapis.com/css2?family=Mulish:ital,wght@0,200..1000;1,200..1000&display=swap" rel="stylesheet" />

</head>
<body>
    
    <main class="page">
      
      <nav class="nav">
    <!-- https://icons.getbootstrap.com -->
    <a class="nav__item " aria-label="Home Page" href="/">
        <svg width="16" height="16" viewBox="0 0 16 16" fill="white" xmlns="http://www.w3.org/2000/svg">
            <path stroke="white" stroke-width="0.5" d="M8.707 1.5a1 1 0 0 0-1.414 0L.646 8.146a.5.5 0 0 0 .708.708L2 8.207V13.5A1.5 1.5 0 0 0 3.5 15h9a1.5 1.5 0 0 0 1.5-1.5V8.207l.646.647a.5.5 0 0 0 .708-.708L13 5.793V2.5a.5.5 0 0 0-.5-.5h-1a.5.5 0 0 0-.5.5v1.293zM13 7.207V13.5a.5.5 0 0 1-.5.5h-9a.5.5 0 0 1-.5-.5V7.207l5-5z"/>    
        </svg> 
    </a>
    
    <a class="nav__item " aria-label="Writing Page" href="/posts/">
        <svg width="16" height="16" viewBox="0 0 16 16" fill="white" xmlns="http://www.w3.org/2000/svg">
            <path stroke="white" stroke-width="0.5" d="m13.498.795.149-.149a1.207 1.207 0 1 1 1.707 1.708l-.149.148a1.5 1.5 0 0 1-.059 2.059L4.854 14.854a.5.5 0 0 1-.233.131l-4 1a.5.5 0 0 1-.606-.606l1-4a.5.5 0 0 1 .131-.232l9.642-9.642a.5.5 0 0 0-.642.056L6.854 4.854a.5.5 0 1 1-.708-.708L9.44.854A1.5 1.5 0 0 1 11.5.796a1.5 1.5 0 0 1 1.998-.001m-.644.766a.5.5 0 0 0-.707 0L1.95 11.756l-.764 3.057 3.057-.764L14.44 3.854a.5.5 0 0 0 0-.708z"/>
        </svg>
    </a>
    
    <a class="nav__item" aria-label="Resume" href="/assets/resume.pdf" target="_blank">
        <svg width="16" height="16" viewBox="0 0 16 16" fill="white" xmlns="http://www.w3.org/2000/svg">
            <path d="M5.5 7a.5.5 0 0 0 0 1h5a.5.5 0 0 0 0-1zM5 9.5a.5.5 0 0 1 .5-.5h5a.5.5 0 0 1 0 1h-5a.5.5 0 0 1-.5-.5m0 2a.5.5 0 0 1 .5-.5h2a.5.5 0 0 1 0 1h-2a.5.5 0 0 1-.5-.5"/>
            <path d="M9.5 0H4a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V4.5zm0 1v2A1.5 1.5 0 0 0 11 4.5h2V14a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z"/>          
        </svg>
    </a>

</nav>

      <header class="page__header">
        
        <h1 class="page__title">Bayesian Modeling - Part 5: Input-Dependent Models and GLMs</h1>
        
        <div style="height: 1rem;"></div><div class="postslist__item__date">Jan 19, 2025</div>
        
			</header>

      <div class="page__content">
        

        <div class="postcontent">
          <p>We’ve journeyed from simple stationary models to complex temporal dependencies. Now we enter the realm of <strong>Dynamic Models</strong> - where external factors influence our generative process. This is where Bayesian modeling becomes incredibly powerful for real-world applications.</p>

<h2 id="linear-regressions-and-the-idea-of-uncertainty">Linear Regressions and the Idea of Uncertainty</h2>

<p>Traditional linear models are based on the equation:
\(y = x \cdot w + c\)</p>

<p>This form represents a <strong>deterministic relationship</strong> between the input $x$ and the output $y$. Every input $x$ maps to a single, exact $y$ without any randomness or uncertainty.</p>

<p>In real-world data, however, outputs are often affected by factors not captured by the model (e.g., measurement errors, hidden variables, or inherent variability). Adding noise $\epsilon \sim \mathcal{N}(0, \sigma^2)$ acknowledges this uncertainty and makes the model more realistic, leading to:</p>

\[y = x \cdot w + c + \epsilon\]

<p>where:</p>
<ul>
  <li>$x$ is the input (features),</li>
  <li>$w$ is the weight vector,</li>
  <li>$c$ is the intercept (bias),</li>
  <li>$\epsilon \sim \mathcal{N}(0, \sigma^2)$ is the Gaussian noise with mean $0$ and variance $\sigma^2$.</li>
</ul>

<p>This can be equivalently <strong>reparameterized</strong> as:</p>

\[y \sim \mathcal{N}(x \cdot w + c, \sigma^2)\]

<p>This directly states that $y$ is drawn from a normal distribution with mean $x \cdot w + c$ and variance $\sigma^2$ (a typical Bayesian representation).</p>

<p>The noisy version better reflects the variability seen in observed data.</p>

<h2 id="example-temperature-dependent-ball-colors">Example: Temperature-Dependent Ball Colors</h2>

<p>What does it mean to be driven by some external factors?</p>

<p>To make the <strong>color of the ball input-dependent</strong>, we can think of <strong>x</strong> as a <strong>temperature scale</strong> (1-dimensional), ranging from a cold “blue” temperature to a warm “red” temperature. The <strong>color of the ball (y)</strong> then depends on this temperature. For instance, at lower temperatures, the balls are more likely to be blue, and as the temperature increases, the likelihood shifts toward red.</p>

<p>Let’s generate some data to illustrate this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">matplotlib.colors</span> <span class="k">as</span> <span class="n">mcolors</span>
<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="c1"># Generate temperature-dependent ball colors
</span><span class="n">w</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">c</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">x</span> <span class="o">*</span> <span class="n">w</span> <span class="o">+</span> <span class="n">c</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>

<span class="c1"># Visualize the data
</span><span class="n">col_line</span> <span class="o">=</span> <span class="n">mcolors</span><span class="p">.</span><span class="n">LinearSegmentedColormap</span><span class="p">.</span><span class="nf">from_list</span><span class="p">(</span><span class="sh">"</span><span class="s">gradient</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">col_line</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">"</span><span class="s">+</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="p">[])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">y (color of ball)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">x (temperature)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Color over temperature</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="model-specification-for-linear-regression-glm">Model Specification for Linear Regression (GLM)</h2>

<p>Let $ x $ represent the temperature, a continuous variable ranging from -10 (coldest) to 10 (hottest).</p>

<p>Modeling the color $y$ of the ball as a linear function of the temperature, we can specify our model as follows:</p>

<p><strong>Priors</strong>: 
\(w \sim \mathcal{N}(0, 1)\) 
\(c \sim \mathcal{N}(0, 1)\)
\(\sigma \sim |\mathcal{N}(0, 1)|\)</p>

<p>Notice how we don’t put any prior on $x$ and only on the parameters - this is the <strong>core of Bayesian modeling</strong>. The prior distributions are initially chosen to be normal with mean $0$ and variance $1$ for simplicity.</p>

<p><strong>Likelihood</strong>: 
\(y \sim \mathcal{N}(x \cdot w + c, \sigma^2)\)</p>

<p><strong>Posterior</strong>:
We will update those priors based on Bayes’ theorem:</p>

\[P(w, c, \sigma | y, x) = \frac{P(y | x, w, c, \sigma) P(w) P(c) P(\sigma)}{P(y)}\]

<p>Where:</p>
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$P(y</td>
          <td>x, w, c, \sigma)$ is the likelihood of observing the data given the parameters.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>$P(w)$, $P(c)$, and $P(\sigma)$ are the prior distributions of the parameters.</li>
  <li>$P(y)$ is the total probability of observing the data.</li>
</ul>

<h2 id="inference-in-linear-regression-with-stan">Inference in Linear Regression with Stan</h2>

<p>Let’s solve this with MCMC using Stan:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tempfile</span>
<span class="kn">from</span> <span class="n">cmdstanpy</span> <span class="kn">import</span> <span class="n">CmdStanModel</span>
<span class="kn">import</span> <span class="n">arviz</span> <span class="k">as</span> <span class="n">az</span>

<span class="c1"># Define the Stan model
</span><span class="n">model_specification</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">
data {
    int&lt;lower=0&gt; N;
    vector[N] x;
    vector[N] y;
}
parameters {
    real w;
    real c;
    real&lt;lower=0&gt; sigma;
}
model {
    // Priors
    w ~ normal(0, 1);
    c ~ normal(0, 1);
    sigma ~ normal(0, 1);
    
    // Likelihood
    y ~ normal( w * x + c, sigma);
}
</span><span class="sh">"""</span>

<span class="c1"># Write the model to a temporary file
</span><span class="k">with</span> <span class="n">tempfile</span><span class="p">.</span><span class="nc">NamedTemporaryFile</span><span class="p">(</span><span class="n">suffix</span><span class="o">=</span><span class="sh">"</span><span class="s">.stan</span><span class="sh">"</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">,</span> <span class="n">delete</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="k">as</span> <span class="n">tmp_file</span><span class="p">:</span>
    <span class="n">tmp_file</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">model_specification</span><span class="p">)</span>
    <span class="n">tmp_stan_path</span> <span class="o">=</span> <span class="n">tmp_file</span><span class="p">.</span><span class="n">name</span>

<span class="c1"># Prepare the data
</span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">N</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">x</span><span class="sh">"</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">y</span><span class="sh">"</span><span class="p">:</span> <span class="n">y</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># Compile and fit the model
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">CmdStanModel</span><span class="p">(</span><span class="n">stan_file</span><span class="o">=</span><span class="n">tmp_stan_path</span><span class="p">)</span>
<span class="n">fit</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">iter_sampling</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Analyze results
</span><span class="n">idata</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="nf">from_cmdstanpy</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span>
<span class="n">az</span><span class="p">.</span><span class="nf">plot_posterior</span><span class="p">(</span><span class="n">idata</span><span class="p">,</span> <span class="n">round_to</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">textsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span><span class="sh">"</span><span class="s">Posterior Distribution of GLM Parameters</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p>Let’s compare our recovered parameters with the true ones:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="k">def</span> <span class="nf">get_color_gradient</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">color1</span><span class="o">=</span><span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">,</span> <span class="n">color2</span><span class="o">=</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">cmap</span> <span class="o">=</span> <span class="n">mcolors</span><span class="p">.</span><span class="n">LinearSegmentedColormap</span><span class="p">.</span><span class="nf">from_list</span><span class="p">(</span><span class="sh">"</span><span class="s">gradient</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="n">color1</span><span class="p">,</span> <span class="n">color2</span><span class="p">])</span>
    <span class="k">return</span> <span class="p">[</span><span class="nf">cmap</span><span class="p">(</span><span class="n">i</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>

<span class="c1"># Compare original vs recovered parameters
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span>
    <span class="sh">"</span><span class="s">Parameter</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">c</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">sigma</span><span class="sh">"</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Type</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="sh">"</span><span class="s">Original</span><span class="sh">"</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="p">[</span><span class="sh">"</span><span class="s">Recovered</span><span class="sh">"</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Value</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span>
        <span class="n">w</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> 
        <span class="n">idata</span><span class="p">.</span><span class="n">posterior</span><span class="p">.</span><span class="n">w</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="nf">flatten</span><span class="p">().</span><span class="nf">mean</span><span class="p">(),</span> 
        <span class="n">idata</span><span class="p">.</span><span class="n">posterior</span><span class="p">.</span><span class="n">c</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="nf">flatten</span><span class="p">().</span><span class="nf">mean</span><span class="p">(),</span> 
        <span class="n">idata</span><span class="p">.</span><span class="n">posterior</span><span class="p">.</span><span class="n">sigma</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="nf">flatten</span><span class="p">().</span><span class="nf">mean</span><span class="p">()</span>
    <span class="p">]</span>
<span class="p">})</span>

<span class="n">sns</span><span class="p">.</span><span class="nf">barplot</span><span class="p">(</span>
    <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> 
    <span class="n">x</span><span class="o">=</span><span class="sh">"</span><span class="s">Parameter</span><span class="sh">"</span><span class="p">,</span> 
    <span class="n">y</span><span class="o">=</span><span class="sh">"</span><span class="s">Value</span><span class="sh">"</span><span class="p">,</span> 
    <span class="n">hue</span><span class="o">=</span><span class="sh">"</span><span class="s">Type</span><span class="sh">"</span><span class="p">,</span> 
    <span class="n">palette</span><span class="o">=</span><span class="nf">get_color_gradient</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">dodge</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">edgecolor</span><span class="o">=</span><span class="sh">"</span><span class="s">black</span><span class="sh">"</span>
<span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Original vs Recovered GLM Parameters</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p>Now let’s visualize the fit with uncertainty:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Visualize the fit with uncertainty
</span><span class="n">col_line</span> <span class="o">=</span> <span class="n">mcolors</span><span class="p">.</span><span class="n">LinearSegmentedColormap</span><span class="p">.</span><span class="nf">from_list</span><span class="p">(</span><span class="sh">"</span><span class="s">gradient</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">col_line</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">"</span><span class="s">+</span><span class="sh">"</span><span class="p">)</span>

<span class="n">p_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="c1"># Plot the uncertainty in the fit
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">w_sample</span> <span class="o">=</span> <span class="n">idata</span><span class="p">.</span><span class="n">posterior</span><span class="p">.</span><span class="n">w</span><span class="p">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
    <span class="n">c_sample</span> <span class="o">=</span> <span class="n">idata</span><span class="p">.</span><span class="n">posterior</span><span class="p">.</span><span class="n">c</span><span class="p">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
    <span class="n">u_y</span> <span class="o">=</span> <span class="n">w_sample</span> <span class="o">*</span> <span class="n">p_x</span> <span class="o">+</span> <span class="n">c_sample</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">p_x</span><span class="p">,</span> <span class="n">u_y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">orange</span><span class="sh">"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># Plot the mean posterior predictive
</span><span class="n">p_y</span> <span class="o">=</span> <span class="n">idata</span><span class="p">.</span><span class="n">posterior</span><span class="p">.</span><span class="n">w</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="nf">flatten</span><span class="p">().</span><span class="nf">mean</span><span class="p">()</span> <span class="o">*</span> <span class="n">p_x</span> <span class="o">+</span> <span class="n">idata</span><span class="p">.</span><span class="n">posterior</span><span class="p">.</span><span class="n">c</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="nf">flatten</span><span class="p">().</span><span class="nf">mean</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">p_x</span><span class="p">,</span> <span class="n">p_y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">black</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">--</span><span class="sh">"</span><span class="p">,</span> 
         <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Mean Post. Predictive</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="p">[])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">y (color of ball)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">x (temperature)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">GLM Fit and Uncertainty in Fit</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p>The orange cloud shows the uncertainty in our linear relationship - this is one of the key advantages of Bayesian GLMs. We don’t just get a single line; we get a full distribution of possible relationships.</p>

<h2 id="input-dependent-mixtures-multiple-factories">Input-Dependent Mixtures: Multiple Factories</h2>

<p><strong><em>IMPORTANT: PLEASE READ THE EXAMPLE</em></strong></p>

<p>Imagine there are <strong>two factories</strong>, each producing balls with specific weight-color patterns:</p>

<ol>
  <li><strong>Factory 1</strong>: Produces balls where heavier balls tend to be redder. The color of the ball increases linearly with its weight but with some variability.</li>
  <li><strong>Factory 2</strong>: Produces balls where heavier balls tend to be bluer. The relationship between weight and color is different and noisier than Factory 1.</li>
</ol>

<p>However, when you receive a shipment of balls, you don’t know which factory produced each ball (this is the <strong>hidden state</strong>). All you see are the weights ($ x $) and colors ($ y $) of the balls.</p>

<p><em>[Image Caption: Two factory diagrams side by side - Factory 1 showing heavier balls getting redder, Factory 2 showing heavier balls getting bluer, with arrows indicating the linear relationships]</em></p>

<h3 id="steps-explaining-the-synthetic-data-generation">Steps explaining the synthetic data generation:</h3>

<ul>
  <li><strong>Step 1</strong>: Randomly assign balls to factories.
    <ul>
      <li>Factory 1 (70% of the time).</li>
      <li>Factory 2 (30% of the time).</li>
    </ul>
  </li>
  <li>
    <p><strong>Step 2</strong>: For each ball, generate a weight ($ x $) uniformly between 1 and 10:
\(x \sim \text{Uniform}(1, 10)\)</p>
  </li>
  <li><strong>Step 3</strong>: Use the corresponding factory’s GLM to generate the ball’s color ($ y $):
    <ul>
      <li>If the ball comes from Factory 1: $ y \sim 0.8 \cdot x + 1.5 + \epsilon $</li>
      <li>If the ball comes from Factory 2: $ y \sim -0.5 \cdot x - 2.0 + \epsilon $</li>
      <li>Where $ \epsilon $ is small Gaussian noise to introduce variability.</li>
    </ul>
  </li>
</ul>

<p>Let’s generate this data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Factory parameters
</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span>
<span class="n">c1</span><span class="p">,</span> <span class="n">c2</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span>
<span class="n">sigma1</span><span class="p">,</span> <span class="n">sigma2</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span>

<span class="c1"># Generate data from each factory
</span><span class="n">x_1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">x_2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">700</span><span class="p">)</span>
<span class="n">y_1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">w1</span> <span class="o">*</span> <span class="n">x_1</span> <span class="o">+</span> <span class="n">c1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma1</span><span class="p">)</span>
<span class="n">y_2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">w2</span> <span class="o">*</span> <span class="n">x_2</span> <span class="o">+</span> <span class="n">c2</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma2</span><span class="p">)</span>

<span class="c1"># Visualize the individual factories
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="n">cmap1</span> <span class="o">=</span> <span class="n">mcolors</span><span class="p">.</span><span class="n">LinearSegmentedColormap</span><span class="p">.</span><span class="nf">from_list</span><span class="p">(</span><span class="sh">"</span><span class="s">gradient</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">])</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x_1</span><span class="p">,</span> <span class="n">y_1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">y_1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">"</span><span class="s">+</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="n">ticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">y (color of ball)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_yticks</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">x (weight)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">From Factory 1</span><span class="sh">"</span><span class="p">)</span>

<span class="n">cmap2</span> <span class="o">=</span> <span class="n">mcolors</span><span class="p">.</span><span class="n">LinearSegmentedColormap</span><span class="p">.</span><span class="nf">from_list</span><span class="p">(</span><span class="sh">"</span><span class="s">gradient</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">])</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x_2</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">y_2</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mi">1</span><span class="o">-</span><span class="n">y_2</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">"</span><span class="s">+</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="n">ticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">y (color of ball)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_yticks</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">x (weight)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">From Factory 2</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span><span class="sh">"</span><span class="s">Truth, Hidden. For understanding only. (notice the colorbars)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="c1"># What you actually observe
</span><span class="n">cmap</span> <span class="o">=</span> <span class="n">mcolors</span><span class="p">.</span><span class="n">LinearSegmentedColormap</span><span class="p">.</span><span class="nf">from_list</span><span class="p">(</span><span class="sh">"</span><span class="s">gradient</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">])</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">tot_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">x_1</span><span class="p">,</span> <span class="n">x_2</span><span class="p">])</span>
<span class="n">tot_y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">y_1</span><span class="p">,</span> <span class="n">y_2</span><span class="p">])</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">tot_x</span><span class="p">,</span> <span class="n">tot_y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">tot_y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">"</span><span class="s">+</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">y (color of ball)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">x (weight)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">What you can Observe</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="mixture-of-glm-specification">Mixture of GLM Specification</h2>

<p><strong>Priors</strong>:</p>
<ul>
  <li><strong>Mixing Weights</strong>:
\(\pi \sim \text{Dirichlet}(\alpha_1, \alpha_2)\)</li>
  <li><strong>GLM Parameters for Each Component</strong>:
\(w_k \sim \mathcal{N}(0, 1), \quad c_k \sim \mathcal{N}(0, 1), \quad \sigma_k \sim |\mathcal{N}(0, 1)| \quad \text{for } k = 1, 2\)</li>
</ul>

<p><strong>Likelihood</strong>:
For each observed $ y_n $:
\(P(y_n | x_n, z_n = k, w_k, c_k, \sigma_k) = \mathcal{N}(x_n \cdot w_k + c_k, \sigma_k^2)\)</p>

<p>The overall likelihood is a mixture:
\(P(y_n | x_n, \pi, \{w_k, c_k, \sigma_k\}_{k=1}^2) = \sum_{k=1}^2 \pi_k \mathcal{N}(y_n | x_n \cdot w_k + c_k, \sigma_k^2)\)</p>

<p><strong>Posterior</strong>:
Using Bayes’ theorem, the posterior is:
\(P(\pi, \{w_k, c_k, \sigma_k\}_{k=1}^2 | y, x) \propto P(y | x, \pi, \{w_k, c_k, \sigma_k\}_{k=1}^2) P(\pi) \prod_{k=1}^2 P(w_k) P(c_k) P(\sigma_k)\)</p>

<p>Let’s implement this mixture of GLMs in Stan:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define the Stan model for mixture of GLMs
</span><span class="n">model_specification</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">
data {
    int&lt;lower=0&gt; N; // Number of data points
    int&lt;lower=0&gt; K; // Number of components

    vector[N] x;
    vector[N] y;
}
parameters {
    simplex[K] pi; // Mixture weights
    vector[K] w; // Slopes
    vector[K] c; // Intercepts
    vector&lt;lower=0&gt;[K] sigma; // Standard deviations
}
model {
    // Priors
    pi ~ dirichlet(rep_vector(1, K));
    w ~ normal(0, 1);
    c ~ normal(0, 1);
    sigma ~ normal(0, 1);
    
    // Likelihood
    for (n in 1:N) {
        vector[K] log_likelihoods;
        for (k in 1:K) {
            log_likelihoods[k] = log(pi[k]) + normal_lpdf(y[n] | x[n] * w[k] + c[k], sigma[k]);
        }
        target += log_sum_exp(log_likelihoods);
    }
}
</span><span class="sh">"""</span>

<span class="c1"># Fit the model
</span><span class="k">with</span> <span class="n">tempfile</span><span class="p">.</span><span class="nc">NamedTemporaryFile</span><span class="p">(</span><span class="n">suffix</span><span class="o">=</span><span class="sh">"</span><span class="s">.stan</span><span class="sh">"</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">,</span> <span class="n">delete</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="k">as</span> <span class="n">tmp_file</span><span class="p">:</span>
    <span class="n">tmp_file</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">model_specification</span><span class="p">)</span>
    <span class="n">tmp_stan_path</span> <span class="o">=</span> <span class="n">tmp_file</span><span class="p">.</span><span class="n">name</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">N</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">K</span><span class="sh">"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">x</span><span class="sh">"</span><span class="p">:</span> <span class="n">tot_x</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">y</span><span class="sh">"</span><span class="p">:</span> <span class="n">tot_y</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">CmdStanModel</span><span class="p">(</span><span class="n">stan_file</span><span class="o">=</span><span class="n">tmp_stan_path</span><span class="p">)</span>
<span class="n">fit</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">iter_sampling</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">idata</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="nf">from_cmdstanpy</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">idata</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">compact</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Notice the label switching!!</strong> All chains look stable so we will stick with one of them to avoid the switching problem.</p>

<p>Let’s visualize the recovered GLMs:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Extract posterior samples (using first chain to avoid label switching)
</span><span class="n">p_pi</span> <span class="o">=</span> <span class="n">idata</span><span class="p">.</span><span class="n">posterior</span><span class="p">.</span><span class="n">data_vars</span><span class="p">[</span><span class="sh">"</span><span class="s">pi</span><span class="sh">"</span><span class="p">].</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">p_w</span> <span class="o">=</span> <span class="n">idata</span><span class="p">.</span><span class="n">posterior</span><span class="p">.</span><span class="n">data_vars</span><span class="p">[</span><span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">].</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">p_c</span> <span class="o">=</span> <span class="n">idata</span><span class="p">.</span><span class="n">posterior</span><span class="p">.</span><span class="n">data_vars</span><span class="p">[</span><span class="sh">"</span><span class="s">c</span><span class="sh">"</span><span class="p">].</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">p_sigma</span> <span class="o">=</span> <span class="n">idata</span><span class="p">.</span><span class="n">posterior</span><span class="p">.</span><span class="n">data_vars</span><span class="p">[</span><span class="sh">"</span><span class="s">sigma</span><span class="sh">"</span><span class="p">].</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Plot the recovered GLMs onto the data
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">mcolors</span><span class="p">.</span><span class="n">LinearSegmentedColormap</span><span class="p">.</span><span class="nf">from_list</span><span class="p">(</span><span class="sh">"</span><span class="s">gradient</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="sh">"</span><span class="s">blue</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">])</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">tot_x</span><span class="p">,</span> <span class="n">tot_y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">tot_y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">"</span><span class="s">+</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Plot the uncertainty
</span><span class="n">x_line</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">50</span><span class="p">):</span>  <span class="c1"># Sample every 50th to reduce clutter
</span>        <span class="n">pi</span> <span class="o">=</span> <span class="n">p_pi</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">g</span><span class="p">]</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">p_w</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">g</span><span class="p">]</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">p_c</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">g</span><span class="p">]</span>
        <span class="n">u_y</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x_line</span> <span class="o">+</span> <span class="n">c</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_line</span><span class="p">,</span> <span class="n">u_y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">orange</span><span class="sh">"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>

<span class="c1"># Plot the mean posterior predictive
</span><span class="n">p_y_1</span> <span class="o">=</span> <span class="n">p_w</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_line</span> <span class="o">+</span> <span class="n">p_c</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">p_y_2</span> <span class="o">=</span> <span class="n">p_w</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_line</span> <span class="o">+</span> <span class="n">p_c</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_line</span><span class="p">,</span> <span class="n">p_y_1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">black</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">--</span><span class="sh">"</span><span class="p">,</span> 
         <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Component 1 Mean</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_line</span><span class="p">,</span> <span class="n">p_y_2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">black</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">-</span><span class="sh">"</span><span class="p">,</span> 
         <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Component 2 Mean</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">(</span><span class="n">sc</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">y (color of ball)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">x (weight)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Mixture of GLMs: Recovered Factory Relationships</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Quiz</strong>: What would happen if we had 3 factories instead?</p>

<h2 id="key-insights-about-input-dependent-models">Key Insights About Input-Dependent Models</h2>

<ol>
  <li><strong>External Influences</strong>: GLMs allow external factors to influence our distributions</li>
  <li><strong>Regression meets Bayesian</strong>: We get uncertainty quantification in our regression parameters</li>
  <li><strong>Mixture Extensions</strong>: We can have different relationships for different subgroups</li>
  <li><strong>Real-world Relevance</strong>: Most real data has input dependencies!</li>
</ol>

<h2 id="whats-next">What’s Next?</h2>

<p>In our final part, Part 6, we’ll combine everything we’ve learned into <strong>Input-Dependent Markov Models (GLM-HMMs)</strong>. These are the most sophisticated models in our series, combining temporal dependencies with input-driven dynamics.</p>

<p>Think of conveyor belts in a factory where the belt choice depends on the previous belt, and each belt produces balls with weight-dependent colors. This is where Bayesian modeling gets truly exciting for complex real-world systems!</p>

<hr />

<p><em>Continue to <a href="../bayesian-tutorial-part6">Part 6: GLM-HMMs and Advanced Sequential Modeling</a> to learn about the most sophisticated models combining all concepts we’ve covered.</em></p>

        </div>
      </div>
      
      <footer class="footer">
        <div class="footer__cp">© 2025</div>
      </footer>

      <!-- 1) Your MathJax configuration -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": { availableFonts: ["TeX"] },
    tex2jax: {
      inlineMath: [['$','$'],['\\(','\\)']],
      displayMath: [['$$','$$'],['\\[','\\]']]
    },
    TeX: {
      extensions: ["AMSmath.js","AMSsymbols.js","color.js"],
      equationNumbers: { autoNumber: "AMS" }
    },
    showProcessingMessages: false,
    messageStyle: "none",
    imageFont: null,
    "AssistiveMML": { disabled: true }
  });
</script>

<!-- 2) Then load the MathJax engine -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script><script>
  if (!(window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1")) {
    (function() {
      var gtagScript = document.createElement('script');
      gtagScript.async = true;
      gtagScript.src = 'https://www.googletagmanager.com/gtag/js?id=G-XWEJZKE72S';
      document.head.appendChild(gtagScript);
  
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-XWEJZKE72S');
    })();
  }
</script>

    
    </main>

  </body>

</html>
